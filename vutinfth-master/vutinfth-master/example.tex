% Copyright (C) 2014-2017 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.
\usepackage{appendix}
\usepackage{rotating}
% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Rebeka Koszticsak} % The author name without titles.
\newcommand{\thesistitle}{Enhancement of Footwear Impressions} % The title of the thesis. The English version should be used, if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
    pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{Bsc}{male}
\setadvisor{Ao.Univ.Prof. Dipl.-Ing. Dr.techn.}{Robert  Sablatnig}{}{male}

% For bachelor and master theses:
\setfirstassistant{Projektass. Dipl.-Ing.}{Manuel  Keglevic}{}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setaddress{Address}
\setregnumber{01325492}
\setdate{04}{01}{2020} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{Enhancement of Footwear Impressions} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
%\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
%\setthesis{bachelor}
%
% Master:
\setthesis{master}
\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Visual Computing}{Visual Computing} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\begin{document}

\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to
%  http://www.informatik.tuwien.ac.at/dekanat

\addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
\addtitlepage{english} % English title page.
\addstatementpage

\begin{danksagung*}
\todo{Ihr Text hier.}
\end{danksagung*}

\begin{acknowledgements*}
\todo{Enter your text here.}
\end{acknowledgements*}

\begin{kurzfassung}
\todo{Ihr Text hier.}
\end{kurzfassung}

\begin{abstract}
\par
Shoeprint images are one of the most commonly secured evidences on crime scenes.
Even though automatic shoeprint processing is a highly researched topic, the final identification is usually done by human forensic experts.
\par
In this thesis possibilities for enhancement of shoeprint samples from a real-life dataset are investigated, and three prototypical approaches are presented and evaluated.
The main challenge of this task is to correctly filter the pattern regardless of the versatile, possibly heavily structured and cluttered noise on the samples.
Among fully automated methods, a semi-automated technique is also tested, where user input is required for noise separation.
\par
The motivation of this work is to give an overview about the potential of the published technologies and to define a universal approach which is able to filter and enhance the shoeprint data despite the presence of noise and the possible low image quality.
Based on the experiences acquired while investigating a new semi-automated noise-suppression and enhancement pipeline for shoeprint images is introduced.
The noisy pixels are identified based on the Fourier-Mellin features of a background pixel given by the user, and of its multi-sized neighborhood.
At the same time a model is built about the appearance of noise, to eliminate that structure from the foreground as well.
Additionally a gradient based line detector is applied and the edge structures of the shoeprint are clustered to distinguish between pattern and noise edges.
The experimental results show that the processed images are clearer, the pattern is sharper whereas the noise is completely eliminated in the background and suppressed in the foreground.
Furthermore,  according to the tests conducted with three basic image descriptor features,  the enhanced shoeprints have higher matching rate to Ground Truth samples than the original images.

\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

\chapter{Introduction}

\par
Shoeprints found on crime scenes are important hints or evidence in a criminal investigation \cite{kong2014novel}.
Even though on one third \cite{alexandre1996computerized} of crime scenes usable shoe patterns can be secured, there is no fully automatized algorithm available yet, which is able to identify and match those prints with the original shoe sole, thus human power is needed \cite{wang2014automatic} to recognize and analyze the found patterns.
The work of forensic experts is not only time consuming and expensive, there is no guarantee about the objectivness of the final outcome\cite{gueham2008automatic}, furthermore the stages of the human matching process are unclear and not necessarily reproducible.
\par
There is an excessive amount of research already done \cite{rida2019forensic} in order to help or replace the work of forensic experts.
However, there is no algorithm published yet, which is reliably usable in varying conditions and sample quality.
One reason for that are the already mentioned versatile conditions, the features and properties of the pattern on the shoe, like age, material, etc., the characteristics of the ground where the shoeprint is left and environmental conditions, like for example weather, highly influence the overall quality of the acquired sample.
Those high amount of factors result in changing appearance of the prints of the same shoe causing high intra class variance while clustering.
Additionally there is a lack of universal, wide ranged database \cite{rida2019forensic} which correctly depicts the common scenarios occurring on real-life crime scenes.
\par
In 2014 a new dataset, called FID-300 \cite{kortylewski2014unsupervised}, was released which aims to solve the database problem described above.
It contains over 1000 reference shoeprint patterns acquired in a laboratory.
Moreover the database introduces 300 new shoeprint samples collected by the police providing an insight on images forensic experts are working with on the daily basis.

\section{Problem Definition}
\par
There are two main stages of automatic shoeprint identification, filtering, where the shoeprint pattern is separated from background and enhanced as well, and matching where the corresponding shoe is determined.
Instead of automatizing the entire shoeprint recognition pipeline this work focuses on the possible ways of increasing the sample quality.
Because of the mentioned absence of general, appropriate database it is difficult to compare the already available methods.
Furthermore it is also challenging to estimate which one is applicable in a real-life scenario.
In this thesis three prototypical enhancing techniques are developed and tested in order to create an overview of the published technologies and their performance and to find a method which handles real crime scene samples universally.
\par
For evaluation and testing the FID-300 database is used.
The dataset contains both in a laboratory acquired, synthetic, as well as on a crime secured, real, shoeprint patterns.
Additionally the Ground Truth about every real sample and its synthetic pair is also available.
The goal of this work is to define an image processing pipeline which correctly identifies and enhances the shoe patterns and eliminates or suppresses the noise on the pattern samples regardless of the quality of the image.
A secondary objective is to gain an overview about the algorithms already published, and make an estimation which methods are applicable in real-life scenarios based on their performance on the FID-300 database.

\section{Challenges}
\par
There are two main obstacles in the topic of shoeprint enhancement and in automatic shoeprint matching in general, the versatile image quality and appearance as well as the lack of universal and wide database.
There are approaches available which build models for given structures of the shoeprint \cite{tang2010footwear}, \cite{alizadeh2017automatic}, but no detailed, uniform representation for the entire shoeprint is available yet to represent the varying shoe pattern appearance.
Moreover there is a high inference of noise from multiple sources.
The produced shoeprint and its background highly depends on the properties of the ground where it was found, the roughness and unevenness of a given type of surface distorts the original pattern and the clutter on the ground appears as noise on the final sample.
Furthermore other objects on the surface, above or behind the left shoeprint additionally cover or distort the original pattern, or prevent to secure the complete area of the original shoe sole.
Besides that the pattern on the original shoe can also be distorted or modified compared to the new version.
Distortions caused by usage are valuable information about the owner, on the other hand they make it more difficult to match the pattern with their unused pairs.
Additional objects between the structures of the shoeprint also alter the original appearance.
Lastly, there are multiple shoeprint securing methods producing different results for the same print \cite{katireddy2017novel}. 
The shoeprint lifting technique used depends on the properties of the ground. 
Those two factors, the securing method and the floor type, also determine if the positive or the negative, the pattern or the space between them, image is captured.
\par
The non-existing universal database causes that the published methods are difficult to compare based on their results since they are using different testing images.
Moreover, the used dataset is not necessarily public \cite{katireddy2017novel}, \cite{dardi2009texture} making it impossible to reproduce the result in those cases.
Additionally the handcrafted databases can be biased, and allow such restrictions and modifications which do not correlate with real-life scenarios \cite{rida2019forensic}.
The used samples are either synthetically generated and computationally distorted \cite{de2005automated}, \cite{gueham2008automatic} or exclude low quality and noisy images \cite{dardi2009texture}, \cite{tang2010footwear}.
Because of that it is difficult to compare their performance and to estimate which one of the approaches are applicable on the FID-300 database.
Furthermore, it is challenging to plan a new algorithm based on the published results because their lack of a uniform baseline.

\section{Contribution}
\par
In this thesis the possible ways of enhancing a shoeprint images are discussed.
Because of the known issues on database multiple approaches are implemented, discussed and evaluated.
The two ways to increase the quality of a given shoeprint sample is to enhance the pattern regardless of the noise and to suppress or eliminate the noise without losing any of pattern information.
Along fully-automated methods semi-automated algorithms are also considered.
Three different approaches are introduced and examined for their performance on real-life image samples.
\par
Finally a new semi-automated framework is given which is evaluated on the FID-300 database.
In the first step user input about the noise is required.
The input is separated into tiles, and the subparts are compared based on the Fourier-Mellin features to the region of the user input.
In that way the background is separated from the foreground and a model about the appearance of the background is calculated.
Since noise appears on the pattern as well, the distorted parts are corrected based on the calculated noise model.
After that gradient based line detection is performed and the results are separated into clusters where pattern and noise classes are defined and candidates of the latter are eliminated.
The final image is thresholded to create a binary image, where the shoeprint is clearly visible and recognizable whereas the clutter is suppressed on the pattern and eliminated on the background.
Throughout the whole processing pipeline morphological operations and small structure elimination is applied multiple times.
First when a mask for background is built, and also in the end of the pipeline to eliminate small inconsistencies on the pattern.
Experimental results show that the enhanced images are clearer, the background is successfully separated and the shoeprint pattern is less noisy than on the original images.
Figure \ref{fig:example} shows an example sample from the FID-300 database \ref{fig:intro:orig} and the enhanced image \ref{fig:intro:enhanced} with the proposed algorithm.
Moreover, the improved images have a better matching rate than their original version according to the experiments conducted on the enhanced images, on the original samples and on the synthetic Ground-Truth using three basic image features such as Fourier-Mellin, SIFT and SURF.


\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00182.jpg}
    \subcaption{Example image from the FID-300 database}
    \label{fig:intro:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{1/00182_filtered.jpg}
    \subcaption{Enhanced image}
    \label{fig:intro:enhanced}
  \end{subfigure}
  \caption{Result of the proposed algorithm}
  \label{fig:example}
\end{figure}

\section{Structure of the Work}
\par
To gain an overview about the research already done the following section, Chapter 2, gives a review of the literature. 
Along papers published on the topic of shoeprint identification, matching and enhancement, research of similar domains is presented as well.
Fields of fingerprint processing and tattoo identification is also overviewed for possibilities of utilizing their solutions in the given problem space.
Furthermore natural image enhancement and denoising techniques are revised as well.
\par
In Chapter 3, 4 and 5 the approaches for enhancement are given and discussed if they are applicable for real-life forensic images.
Chapter 3 presents and evaluates an algorithm for detecting shoe pattern on varying conditions.
Chapter 4 describes an automated noise suppression pipeline.
In Chapter 5 the new algorithm for enhancing real-life crime scene shoeprint impressions is proposed.
Details on implementation are revealed as well.
\par
In Chapter 6 experimental results are shown and the proposed algorithm is evaluated.
In Chapters 7 and 8 prospective future work is discussed and the final conclusion is given. 

\chapter{Related Work}
\par
In order to find and develop an effective algorithm for shoeprint enhancement an overview about relevant research is made first.
Along the evident literature of image enhancement and noise removal a related topic, discriminative image descriptors, is also considered, to gain the best insight possible and to define an approach which is optimized for the rest of the shoeprint identification pipeline.
In this chapter the research on the domain of shoeprint identification is reviewed.
Other than that publications from similar domains such as fingerprint and palmprint detection as well as tattoo identification are described.
Research for fingerprint identification and tattoo recognition have been chosen for review because of their similar goal of edge structure and minimal image structure recognition.
Moreover an overview of techniques from the field of natural image enhancement and description along with general image denoising is also given.
This chapter is separated into three parts, first Image Enhancement techniques are described, after that algorithms developed for Noise Removal specifically are discussed, and lastly proposed Image Descriptors are reviewed.


\section{Image Enhancement}
\label{sec:rw:ImageENhancement}

In this section image enhancement techniques from four specific domains are discussed, these are shoe- and fingerprint identification, tattoo recognition and natural image enhancement.

\section*{Shoeprint Enhancement}

\par
There is an extensive research done in the field of enhancing shoeprint images.
However,  the problem definition and the use-case of the different publications varies strongly.
Because of the absence of standard database, the discussed algorithms can be separated into two groups, techniques tested on synthetic samples and on real-life impressions.
Synthetic samples are scanned in images in a laboratorial environment for the purpose of building a dataset for shoeprint identification.
The noise derives from scanning artifacts and computationally added distortions and modification.
Furthermore many algorithms developed for real crime-scene data make restrictions about the input image and exclude noisy and poor quality images.
Figure \ref{fig:rw:database} shows example images from a synthetic \ref{fig:rw:synthetic}, from a restricted \ref{fig:rw:restricted} and high \ref{fig:rw:highFID} and low quality samples \ref{fig:rw:lowFID} from the FID-300 dataset.
In the following discussion it is noticed repeatedly, which kind of dataset the proposed approach was tested on.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/synthetic.jpg}
    \subcaption{Example image from a synthetic dataset}
    \label{fig:rw:synthetic}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/restricted.jpg}
    \subcaption{Example image from a real crimescene dataset excluding low quality images}
    \label{fig:rw:restricted}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/00025.jpg}
    \subcaption{Example high quality image from the FID-300 database}
    \label{fig:rw:highFID}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/00174.jpg}
    \subcaption{Example low quality image from the FID-300 database}
    \label{fig:rw:lowFID}
  \end{subfigure}
  \caption{Example images of a synthetic \cite{alizadeh2017automatic}, of a restricted \cite{li2014retrieval} and of the FID-300 \cite{kortylewski2014unsupervised} dataset}
  \label{fig:rw:database}
\end{figure}

\par
Morphological Operations, Thresholding and Image Filtering are popular techniques for improving the quality of both kind, realistic and synthetic, of input data.
Morphological Operations, especially Opening and Closing, is used in many cases \cite{wang2014automatic}, \cite{kong2014novel}, \cite{li2014retrieval}, \cite{tang2010footwear}, \cite{wu2019crime}, where Wang et al. \cite{wang2014automatic} uses a synthetic dataset, and other than Wu et al. \cite{wu2019crime} the forensic images are restricted to high quality data.
Wang et al. \cite{wang2014automatic}, Kong et al.  \cite{kong2014novel} and Li et al. \cite{li2014retrieval} use the Morphological Operations to correct inconsistencies after thresholding.
Similar to the previous approaches Wu et al. \cite{wu2019crime} applies the same pipeline on a real forensic dataset.
Tang et al. \cite{tang2010footwear} follow the same principle but instead of thresholding, after Canny edge detection is Opening and Closing used.
\par
To create a binary image and eliminate noise various thresholding techniques are used.
Otsu  \cite{wu2019crime}, \cite{algarni2008novel}, \cite{alizadeh2017automatic}, \cite{kong2014novel} and adaptive thresholding \cite{wang2014automatic}, \cite{li2014retrieval} are the two most popular algorithms.
Algarni et al. \cite{algarni2008novel} and Alizedah et al. \cite{alizadeh2017automatic} along with Wang et al. \cite{wang2014automatic} published their algorithms for synthetic datasets.
Kong et al. \cite{kong2014novel} and Li et al. \cite{li2014retrieval} tested on restricted, whereas Wu et al. \cite{wu2019crime} developed their approach for full real forensic database.
Wang et al. \cite{wang2014automatic} and Wu et al. \cite{wu2019crime} combine thresholding with a grid based approach to calculate exact thresholds for every subarea of the picture.
\par
Another way to eliminate noise is image filtering.
Alizadeh et al. \cite{alizadeh2017automatic} uses a simple Median filter on a synthetic dataset.
Zhang et al. \cite{zhang2005automatic} test on synthetic database as well, and they take advantage of the Partial Differential Equations approach.
In this way the edges are preserved while the background is smoothed according to a controlled curvature motion criteria. 
Katireddy et al. \cite{katireddy2017novel} uses Successive Mean Quantization Transform (SMQT) \cite{nilsson2013smqt} as an only step to enhance a real-life database.
Figure \ref{fig:rw:SMQT} shows the output of the SMQT algorithm on an example image.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/SMQTorig.jpg}
    \subcaption{Example shoeprint impression}
    \label{fig:rw:SMQTin}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/SMQT.jpg}
    \subcaption{Enhanced image with the SMQT algorithm}
    \label{fig:rw:SMQTout}
  \end{subfigure}
  \caption{Example image about the enhancement feature of the SMQT algorithm \cite{katireddy2017novel} }
  \label{fig:rw:SMQT} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
Bandpass operators are also used for noise suppression.
The images are converted to the frequency domain where high and low frequencies are eliminated.
Gueham et al. \cite{gueham2007automatic} and Richetelli et al. \cite{richetelli2017classification} utilize this method on a synthetic database.
Li et al. \cite{li2014retrieval} work with a restricted real dataset, where only the lower frequencies are eliminated.
Another frequency based approach was proposed by Katireddy et al. \cite{katireddy2017novel} for real dataset based on Daubechies wavelets
After SMQT enhancement the Daubechies wavelets are used to separate the fore- and background and to remove the noise in the latter.

\section*{Fingerprint Enhancement}

Bandpass and general image filtering is popular in the field of fingerprint enhancement as well.
Zhou et al. \cite{zhou2011adaptive} uses a low- and a highpass filter to eliminate striking frequencies. 
Baig et al. \cite{baig2015enhancement} apply Directional Hilbert transform of Butterworth bandpass to collect the different phase shifts and eliminate the artifacts created by previous thresholding of the input.
Wang et al. \cite{wang2014enhanced} decompose the image into four subbands and process them separately, calculating the noise for every subband respectively.
Li et al. \cite{li2012texture} use Fourier transformation combined with Scale Invariant Feature Transform (SIFT) \cite{lowe1999object} to enhance the fingerprint images. 
With SIFT the interesting points in the Fourier domain are found and secured, while the image is filtered to suppress noise and eliminate other inconsistencies.
Jahan et al. \cite{jahan2017robust} apply Fuzzy filtering followed by thinning.
Fuzzy filter is a local method to preserve the edge information and fine line structures and suppress the noisy background of the input.

\section*{Tattoo Enhancement}

For tattoo enhancement an algorithm from Han et al. \cite{han2013tattoo} was proposed which combines Gaussian filtering with Hysteresis thresholding. 
Hysteresis thresholding is a neighborhood-aware approach where a pixel is labelled when it is above a given low threshold and simultaneously connected to other pixels meeting a higher thresholding criteria.
Acton et al. \cite{acton2008matching} propose to use Active Contour Model to find the boundaries of tattoo images and apply Opening and Closing as well to get rid of small inconsistencies.

\section*{Natural Image Enhnacement}
\par
Along Signal, especially Bandpass, general Image Filtering and Thresholding, Histogram and Color Operations are also common for natural image enhancement.
Maini et al. \cite{maini2010comprehensive} published a review about natural image enhancing algorithms and defined two main groups of algorithms, Frequency and Spatial Domain Methods.
First publications utilizing techniques from the first group are discussed, after that the usage of Spatial Domain Methods is reviewed.
\par
Xu et al. \cite{xu2016image} combines Bandpass filtering with adaptive thresholding.
Similar to Wang et al. \cite{wang2014enhanced} the image is separated into four subbands, and the threshold is calculated for every image separately.
Sugamya et al. \cite{sugamya2016image} applies Subband Decomposition with two staged Histogram Equalization.
The histogram of the input is equalized globally first, after that it is decomposed into subbands to equalize the values locally for every four generated subimage.
\par
Median Filters are used not only in the domain of shoeprint enhancement \cite{alizadeh2017automatic} but also for natural image noise suppression.
Apart from Median Filter Li et al. \cite{li2014rapid} utilize Average and Wiener Filter as well to suppress the occuring noise and to prepare the input for neighborhood based feature extraction.
Feng et al. \cite{feng2011bag} proposed a Bag-of-Words algorithm based on the Gabor wavelets of the input. 
For preprocessing the Watershed Transform is used.
\par
Histogram Operations are combined apart from Bandpass filtering as Sugamya et al. \cite{sugamya2016image} do but also with Thresholding as proposed by Yao et al. \cite{yao2016image}.
Their approach separates the histogram of the input into two parts according to Otsu\'s method, after that the histogram  of the generated subimages is equalized.
Figure \ref{fig:rw:BHNMT} shows the results of algorithm on an example image.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/BHNMTorig.jpg}
    \subcaption{Example low-contrast image}
    \label{fig:rw:BHNMTin}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{2/BHNMT.jpg}
    \subcaption{Enhanced image with the histogram thresholding algorithm}
    \label{fig:rw:BHNMTout}
  \end{subfigure}
  \caption{Example image  \cite{yao2016image} about the enhancement feature of the algorithm proposed by Yao et al. \cite{yao2016image} }
  \label{fig:rw:BHNMT} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
Color processing techniques are widespread in the topic of natural image enhancement.
It is used for image dehazing for low contrast images, \cite{singh2018dehazing} and also for classical noise removal \cite{ren2018joint}, \cite{zhang2016simultaneous}. 
Bhairannawar et al. \cite{bhairannawar2017color} switch from RGB to HSV and use Laplace filter to detect regions with intensity changes. 
During processing the H channel is not modified to prevent color distortion artifacts.
Although color processing is a well researched field with many promising solutions, no wider overview is given in this thesis.
Even though there are shoeprint impression datasets with colored samples \cite{katireddy2017novel}, FID-300 provides grayscale images thus no color processing approach can be used in this specific case. 

\section{Noise Removal}

Noise Removal methods are based on estimating the original image, eliminating the deviating features of the data.
One way is denoising through gradient histogram preservation \cite{zuo2013texture}.
The distribution of gradients is estimated on the original image, and the noisy image is adjusted to the calculated values.
An alternative way is to decompose a single image and based on the clear parts the noisy regions are approximated.
Huang et al. \cite{huang2013self} propose a self-learning algorithm which only considered the high frequency parts of the decomposed image.
There were several approaches published, where the images are separated spatially instead of the frequency domain.
\cite{xu2015patch}, \cite{talebi2013global}, \cite{chatterjee2011patch} and \cite{guo2015efficient} are techniques based on the idea of non-local means, where the image is subsampled into tiles and the pixels are set to the mean of regions belonging to the same cluster.
The main difference between the previous algorithms is how they classify the image subregions into different classes.
Taleby et al. \cite{talebi2013global} uses an iterative shrinkage strategy. 
Chatterjee et al.  \cite{chatterjee2011patch} groups the geometrically similar regions and estimate the noise for every class separately with the Wiener filter, whereas Guo et al. \cite{guo2015efficient} utilize Block Matching to determine the cluster memberships. 
Additionally the spatial location is also considered while calculating the mean value in a given class.
The members are weighted according to the distance to the current region.

\section{Image Description}

Similar to the previous Image Enhancement section \ref{sec:rw:ImageENhancement} this part is also subdivided into four domains offering solutions for the same problem in different areas.
In this section the published image features are described which are proposed to be the most discriminative for their field.
Similar topics are reviewed to gain insight about the most powerful descriptors and consider to port them into the domain of shoeprint enhancement.
Shoeprint descriptors are described first, after that fingerprint features are discussed.
Following that an overview about tattoo and natural texture descriptors is given at the end.

\section*{Shoeprint Descriptors}
\par
Shoeprint Description  in varying difficulty and quality of the different datasets is a continuous issue in the research of Shoeprint Identification, therefore the properties of the database the given approach was tested on is noted repeatedly.
Signal or frequency domain based image features are popular in both groups of algorithms using synthetic or real samples for testing.
Gabor Transform is used in several applications tested on synthetic \cite{patil2009rotation}, on restricted \cite{kong2014novel}, \cite{li2014retrieval} and on real forensic data \cite{wu2019crime} as well.
Patil et al. \cite{patil2009rotation} propose to use the Radon Transform to determine the dominant direction of structures on the print and process the aligned image with the Gabor Transform.
Kong et al.  \cite{kong2014novel} combined the Gabor features of the sample with Zernike features to describe the shapes in the pattern.
Li et al. \cite{li2014retrieval} suggest to use the histogram extracted in the Gabor transformed domain as descriptors.
In the approach published by Wu et al.  \cite{wu2019crime} Gabor Filters are combined with Haar Wavelets and Fourier-Mellin Transform to get an integrated, multi-level descriptor. 
There are other publications along that where Fourier-Mellin Transform is proposed for feature description.
Gueham et al. \cite{gueham2008automatic} use the classical Fourier-Mellin pipeline to compare samples from a synthetic dataset.
As Wang et al. \cite{wang2014automatic} state, Fourier-Mellin transform allow multi resolution matching, so they apply it successfully on synthetic data.
Richetelli et al. \cite{richetelli2017classification} classify synthetic shoeprint impressions by applying Fourier-Mellin transform following the calculation of Phase Only Correlation (POC) to determine the translative difference between two images in the frequency domain.
Gueham et al. \cite{gueham2007automatic} in another paper suggest to use the basic Fourier transform before calculating the POC.
Unlike the approach of Gueham et al. \cite{gueham2007automatic}, which were only tested on synthetic images, Kortylewsky et al. \cite{kortylewski2014unsupervised} propose a Fourier Transformation based method for real forensic images.
It is an unsupervised learning approach, where the periodic structures on a shoeprint are compared in the Fourier domain.
Richetelli et al. \cite{richetelli2017quantitative} compares Randomly Acquired Characteristics of shoeprints, e.g. small damages, modifications and stuck objects on or in the shoesole pattern, examining their Fourier descriptor.
Other than Fourier-like transformations the use of Power Spectral Density was also proposed for a restricted dataset \cite{dardi2009texture}.
It is a descriptor for random, broadbrand signals based on the frequency and power.
\par
In high quality datasets shape descriptors are a popular choice for feature extraction.
Algarni et al. \cite{algarni2008novel} suggest to use Hu moments because of its robustness against noise, rotation and resolution.
For restricted databases it is proposed to combine the feature descriptors, Kong et al. \cite{kong2014novel} incorporate Zernike and Gabor features whereas Tang et al. \cite{tang2010footwear} define their own fundamental shapes based on common basic structures on a shoe sole through Hough Line Transform.
The extracted features are then stored in an Attributed Relational Graph to represent the entire shoeprint image.
\par
SIFT and Harris Detector are the two most popular point descriptors for matching shoeprint images.
Nibouce et al. \cite{nibouche2009rotation} propose to use a four-level Harris and combine with SIFT on a synthetic database.
Almaadeed et al. \cite{almaadeed2015partial} use the same combination and the Hessian Detector additionally for the same purpose on a restricted real-life dataset.
Another publication \cite{richetelli2017classification} extract the SIFT features from the frequency domain after applying Fourier-Mellin Transformation and POC on the input image.
\par
Kong et al. \cite{kong2017cross}, \cite{kong2019cross} define a new descriptor for real database extracting the mid-level features from a Convolutional Neural Network.
Wu et al .\cite{wu2019losgsr} also use machine learning to calculate opinion scores for given matching pairs from real forensic data.
In the learning process manifold ranking is used where he opinions of human experts as well as feature similarities are both incorporated.
Kortylewsky et al. \cite{kortylewski2016probabilistic} developed hierarchical composition for Active Basis Models for the same real database, FID-300, as used in this thesis, and also extended for natural image environments \cite{kortylewski2019greedy}.
Sparse representation was also proposed \cite{alizadeh2017automatic} this method however was only tested on synthetic data and not on real forensic dataset.

\section*{Fingerprint Descriptors}
\par
Signal domain representations, because of their robustness against rotation, are attractive not only for shoeprint but also for fingerprint description.
Multi-resolution representation through Gabor expansion was proposed by Bolle et al. \cite{bolle2012fingerprint} to get a localized texture descriptor.
Van et al. \cite{van2016fingerprint} use Adaptively Adjusted Modified Finite Radon Transform after Gabor filtering to connect edges and eliminate inconsistencies.
Rida et al. \cite{rida2018palmprint} propose an ensemble descriptor consisting of Gabor filter, Local Binary Pattern, Histogram of Oriented Gradient and Line detector to represent a palmprint impression.
Other than that, Li et al. \cite{li2012texture} suggest to use Directional Filter Banks, where the image is separated into eight directions and every subimage is decomposed into two frequencies via Wavelet Transform.
\par
Unlike in the domain of shoeprint image representation, in case of fingerprint images several point feature descriptors were proposed.
In \cite{zhou2011adaptive} SIFT features were fused with Hough Transform and Minutiae Information of the fingerprint.
Chen et al. \cite{chen2013hierarchical} also use the Hough Transform and extends with hierarchical voting score to get better matching information.
Along  \cite{rida2018palmprint} Ghandehari et al. \cite{ghandehari2012palmprint} recommend to use HOG in a 3-level Gaussian pyramid to estimate the local strength of different kind of edges on the image.
Jahan et al. \cite{jahan2017robust} suggest to combine the Minutiae Information with Speeded Up Robust Features and compare them with a Neural Network to find the matching pairs.
\par
For fingerprint description Local Binary Patterns (LBP) are also suitable.
As mentioned previously Rida et al. \cite{rida2018palmprint} published a combined feature vector which LBP is also a part of.
Wang et al. \cite{wang2013pixel} modify the usual LBP pipeline with Pixel to Patch sampling to increase the quality of descriptor without slowing down the calculation.
In Pixel to Patch sampling the weighted average of the neighboring pixels in a given radius is calculated instead of interpolation.
Additionally the Local Neighboring Intensity Relationships based on gray-scale information are also considered.
\par
Sparse representations are also popular for fingerprint description.
Rida et al. \cite{rida2018palmprint} stores the hybrid features in that way, whereas Shao et al. \cite{shao2013fingerprint} represents predefined fingerprint patches, called dictionary atoms, in a sparse way.

\section*{Tattoo Descriptors}
\par
For tattoo description three main methods were introduced, signal domain features, point descriptors, especially SIFT, and shape features and Kim et al. \cite{kim2015robust} fuses all three of them.
For local descriptor the Shape Context Features are used whereas for global descriptor multi-level SIFT and the Fourier Transform is utilized.
Acton et al. \cite{acton2008matching} build an Active Contour Model which consists of Haar Wavelet, an HSV histogram and a Fourier Shape Descriptor.
\par
Other than  \cite{kim2015robust} there are multiple publications available, e.g. \cite{duangphasuk2013tattoo}, which take advantage of SIFT features for tattoo image description.
It is common to combine them with other shape descriptor to have a more detailed representation.
Han et al. \cite{han2013tattoo} fuse Active Shape Models with SIFT,  in \cite{yi2015impact} SURF is added and in \cite{kim2016tattoo} SIFT is extended with the Local Self Similarity measure.
Duangphasuk et al. \cite{duangphasuk2013tattoo} base their approach mainly on shape description and similar to  \cite{kim2015robust} use Shape Context Features for tattoo representation.

\section*{Natural Texture Descriptors}
\par
For natural texture description signal domain features and LBPs are frequently used.
Mistry et al. \cite{mistry2017content} mix multiple features, mainly color and texture descriptors.
For texture description Gabor Wavelet and Binarized Statistical Images  \cite{kannala2012bsif} are used simultaneously.
Hatipoglu et al. \cite{hatipoglu2000image} suggest to take advantage of Dual Tree Complex Wavelet transform instead of Gabor Wavelet.
Quevedo et al. \cite{quevedo2002description} and Xu et al. \cite{xu2009viewpoint} suggest Fractal features, because they are invariant to intensity and scale changes.
Xu et al. \cite{xu2009viewpoint} propose to use multifractal spectrum to achieve robustness against viewpoint changes and non-rigid deformations additionally.
Hayati et al. \cite{hayati2018wirif} and Ahonen et al. \cite{ahonen2009rotation} follow the same principle by incorporating LBP-like features with the frequency domain representation.
Ahonen et al. \cite{ahonen2009rotation} calculate the LBP of the Fourier features of the image, whereas Hayati et al. \cite{hayati2018wirif} use Wave Inference, where the information of multiple different-sized asymmetric neighborhoods is added respectively. 
\par
There are several publications available which use LBPs for texture description, e.g. \cite{guo2012discriminative}, \cite{hong2014combining}, \cite{ahonen2009rotation}, or for texture classification, e.g. \cite{khellah2011texture}, \cite{guo2010rotation}, \cite{zhang2017learning}.
It is common however, that LBP is used combined with other techniques to create a discriminative and detailed descriptor.
In \cite{hong2014combining} based on the covariance matrix additional discriminative features are calculated. 
As mentioned previously Ahonen et al. \cite{ahonen2009rotation} use the LBP features of the Fourier domain, similarly in \cite{guo2010completed} LBP features are calculated twice, after the input is separated into two components, into signs and magnitudes, to make the basic LBP rotation invariant as well.
Zhang et al.  \cite{zhang2017learning} propose a learning strategy for adaptively weighting the sign and magnitude LBPs to estimate their contribution in the given area. 
A third modified LBP is also defined, where the local difference vector is determined, in this way robustness against illumination changes is achieved.
In \cite{khellah2011texture} LBP is calculated on multiple levels.
Another solution for rotation invariance is proposed by Davarzany et al. \cite{davarzani2015scale}, in their approach a circular neighboring radius and dominant orientation is stored additionally so that scale invariance is also granted.
Li et al. \cite{li2014rapid} process the neighborhood with Rapid Transform, which is robust against cyclic permutation, to achieve the same robustness.
Wang et al. \cite{wang2017local} suggest to solve this problem by storing the radial and tangential information instead of intensity values.
Guo et al. \cite{guo2010rotation} defines LBPV instead of LBP where V stands for variance. 
In their approach the LBP features with high variance are chosen as discriminative, because they indicate high frequency in the related region.
In \cite{bala2016local} it is proposed to calculate Local Texton Patterns, where Value chanel of the HSV input is subdivided into overlapping subblocks according to its content, and the modified LBP is then determined based on those subregions.
Fadaei et al. \cite{fadaei2017local} published a similar approach called Local Derivative Radial Patterns.
Instead of binary coding, multi-level coding is used in different directions, and the differences between neighbors are weighted additionally.
Chahi et al. \cite{chahi2018local} define Local Ternary Patterns which store the directional patterns explicitly.
\par
In \cite{kannala2012bsif} a new LBP similar descriptor is defined called Binary Statistical Image Feature (BSIF) which is also proposed to use by Crossier et al. \cite{crosier2010using}.
In BSIF prelearned filters are used and the responses are stored in the feature, it can handle large intra-class variation when used for classification, however it varies greatly when the scale changes.
For that reason Crossier et al. \cite{crosier2010using} suggest to calculate the features on multiple resolution to gain scale invariance as well.
\par
Varied Local Edge Pattern Descriptors (VLEP) \cite{yan2016edge} are used to represent edge information.
Every pixel of an edge is described by the angle and the magnitude of the gradient which stand for edge direction and strength respectively.
Wang et al. \cite{wang2018using} extend the feature to be scale invariant by combining two or more modified VLEPs, one calculated on a different scale and another calculated on different scale and resolution.

\chapter{Pattern Detection}
\par
In this and in the following two chapters three different approaches are introduced and discussed if they are usable for enhancement of real-life footwear impression.
As mentioned earlier the testing data of already published algorithms varies greatly, thus it is difficult to make an estimation about their performance on the FID-300  \cite{kortylewski2014unsupervised} dataset.
For that reason three prototypical application are developed and evaluated in this thesis, and the one with most promising results is further elaborated.
The goal of these three chapters is to give an overview about possible directions of development and to provide information about the effectiveness of the given techniques on a real-life forensic dataset.
\par
There are two ways to increase the quality of a given input, find and enhance the information regardless of the properties and presence of noise or suppress any kind of noise preserving the information.
In this chapter an experimental application for the first possibility is introduced in Chapter 4 an approach for noise suppression are discussed.
In Chapter 5 a semi-automated enhancement algorithm is presented.
The theoretical background and the structure is discussed in the first section, afterwards implementation details are reported followed by the evaluation of the results.

\section{Methodology}
\par
To find the shoeprint pattern in any input regardless the noise a robust and discriminative descriptor is needed.
Since the noise and other inference is highly variable in real-life settings the impressions from the same shoe can greatly differ depending on the circumstances causing high intra-class variance.
On Figure \ref{fig:pe:database} four samples from the same shoeprint are shown, noise occurs not only in the background but also on the shoeprint itself, the appearance of noise changes within one image and illumination changes occur as well.
Furthermore on three examples only a partial shoeprint is depicted and the size and resolution of the samples also varies.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00009.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00020.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00021.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00066.jpg}
  \end{subfigure}
  \caption{Example impressions of the same shoe from the FID-300 \cite{kortylewski2014unsupervised} dataset}
  \label{fig:pe:database}
\end{figure}

\par
For that reason a discriminative feature learning algorithm is proposed based on the work of Guo et al. \cite{guo2012discriminative}.
In their work a three-layered Local Binary Pattern (LBP) feature learning algorithm is introduced for natural texture description, the schematic workflow the algorithm is shown on Figure \ref{fig:pe:workflow}
They utilize on feature selection where only the most discriminative descriptors are chosen and propagated further.
In the first layer, where robustness is granted, the features are selected which describe the majority of given texture, by finding the frequently appearing descriptors and ignoring the rare ones since they are more sensitive to noise.
For that a threshold is defined, all descriptors of the given texture are selected in the order of their frequencies starting with the most frequent one.
If the already selected descriptors cover a bigger region of the original texture than the given threshold, the selection process stops and the chosen descriptors are propagated for the next level.
The selection of threshold is crucial in this process, if it is too high, less robust descriptors with low frequency are also selected, when the threshold is too low, only the most frequent descriptors are considered and valuable details of the texture are ignored.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{3/workflow.jpg}
  \caption{The workflow of the three-layered feature learning algorithm  \cite{guo2012discriminative}.}
  \label{fig:pe:workflow} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
The following two layers work according to Fisher\'s Separation Criteria, the second layer aims to minimize intra-class variance, whereas the third layer ensures maximum inter-class distance. 
The second layer provides discriminative power.
It gathers all selected descriptors from the previous layer of the same texture on different samples and intersects those sets.
The main assumption is that the same texture have similar properties under varying circumstances.
In this layer features are selected which occur in every sample of the given texture.
\par
In the last, third layer representation capability is ensured.
This layer merges all remaining features from the second layer into one set to create a dominant feature pool.
In the second layer a feature set for a given texture is created based on the extracted descriptors from all samples of the same texture, in the third layer those feature sets of every occuring textures are collected and united.
\par
For the purpose of this prototypical implementation only the first two layers is used, because the goal is to distinguish between noise and information, two classes, pattern and noise, are defined.
Since the noise has no regularity along the samples, descriptors for shoe patterns are only learned.
The primary assumption is that there is a fundamental similarity between every shoeprint image, e.g. regular line structures, recurring basic patterns, which is detected and learned.
Guo et al. \cite{guo2012discriminative} proposed their algorithm for LBP features, and since it is a popular descriptor for natural textures \cite{hong2014combining}, \cite{ahonen2009rotation} and fingerprints \cite{wang2013pixel}, \cite{rida2018palmprint}, LBP is chosen to be a candidate descriptor for shoeprint detection.
In the research of shoeprint identification both frequency based feature descriptors, such as Fourier-Mellin Transform \cite{wu2019crime}, \cite{gueham2008automatic}, and SIFT \cite{nibouche2009rotation}, \cite{richetelli2017classification} are commonly used descriptors, thus those are also selected for feature learning.
\par
The three-layered learning algorithm is implemented for three different feature descriptors, LBP, Fourier-Mellin and SIFT, but the learning process follows the same steps as proposed by Guo et al. \cite{guo2012discriminative}.
In the first layer Guo et al. \cite{guo2012discriminative} propose a threshold based on the area the selected features cover on the original image.
However this criteria is altered in the case of LBP and SIFT to adjust the learning algorithm to the needs of this research.
Three-layered learning was developed originally to find a discriminative feature set for several texture images from a wide database, from \cite{ojala2002outex}, \cite{dana1999reflectance}, \cite{boland2001neural}, \cite{jantzen2005pap} and \cite{brahnam2007introduction}, but in this project no such dataset is available.
Furthermore the goal is to distinguish between noise and shoeprint and not to find discriminative features within multiple texture images.
For that reason features are firstly extracted from both fore- and background of the samples, where the foreground contains the exact shoeprint pattern and the background is the noisy ground where the shoeprint is lying.
Since there is no labelled data in the FID-300 \cite{kortylewski2014unsupervised} dataset available, the samples chosen for training were labelled manually.
Figure \ref{fig:pe:mask} shows two example images the black regions of the mask show the shoeprint information whereas the white pixels indicate the background.
After that, to achieve high inter-class distance despite the absence of the third layer, frequent descriptors of the noise are determined and all of them are eliminated from the pattern descriptors.
The remaining pattern descriptors are propagated to the next level. 
To have a reasonable learning time in case of Fourier-Mellin Transform no such modification is made.
After all descriptors are calculated those are selected which occurrence exceed a given threshold.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00009.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00009_mask.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00025.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00025_mask.jpg}
  \end{subfigure}
  \caption{Examples from FID-300 \cite{kortylewski2014unsupervised} dataset chosen for three-layered learning and their corresponding manually labelled masks.}
  \label{fig:pe:mask}
\end{figure}

\par
The easiest way to determine the occurrences of one descriptor is to calculate the complete matches within one sample.
This method however results in high amount of descriptors with little number of occurance.
For that reason, a similarity measure is used for all three candidate features, if the similarity value reaches a predefined threshold the two descriptors are considered as the same, they are merged and the number for occurance is incremented.
For LBP histogram correlation is used for comparison, for SIFT Brute-Force matcher is applied and for the Fourier-Mellin Transform the similarity measure proposed in \cite{gueham2008automatic} used.
The correlation coefficient between two Fourier-Mellin descriptors is given by:
\[ corr(FM_{1},FM_{2}) = \frac{\sum\limits_{m}\sum\limits_{n}(FM_{1mn}-\overline{FM_{1}})(FM_{2mn}-\overline{FM_{2}})}{\sqrt{(\sum\limits_{m}\sum\limits_{n}(FM_{1mn}-\overline{FM_{1}})^2)(\sum\limits_{m}\sum\limits_{n}(FM_{2mn}-\overline{FM_{2}})^2)}}  \]
\label{FMcorr}
where $FM_{1}$ and $FM_{2}$ are two Fourier-Mellin Features to compare and $\overline{FM_{1}}$ and $\overline{FM_{2}}$ are the mean values of the corresponding features.
\par
At the end of the learning process a descriptor pool is created.
The shoeprint area of a new input is determined based on those learnt features using the same similarity measures as in the learning phase.

\section{Implementation}
\par
In this section implementation details of the modified three-layered learning algorithms are revealed and parameter settings are specified.
This and the following algorithms are written in Python 2.7 \cite{van1995python} using OpenCV 4.1.1 \cite{opencv_library}.
Being a prototypical implementation, the values of the variables stated in this section are determined through experiments.
\par
When starting the application the training data is read.
It consists of the original shoeprint images and mask images where the relevant regions of the corresponding shoeprint is marked.
The two sets of pictures are stored in a vector, it is assumed that the mask has the exact same size as the corresponding shoeprint image, that every image has a mask and that the images and masks are stored in the same order.
For training a limited amount of seven images were selected and labelled manually.
The FID-300 \cite{kortylewski2014unsupervised} dataset consists of 300 real-life forensic samples and 1175 synthetic samples, thus not every synthetic sample has a real pair, and the majority of synthetic images with corresponding real samples have only one or two matching pairs in the database.
The chosen seven images depict shoeprint images from the same shoe, one of them is a synthetic sample the rest of them is a real one.
\par
To speed up the shoeprint recognition, the learnt features are stored in a \texttt{.txt} file.
It is determined if a new learning process starts or the already calculated descriptors are read from file.
When the learning starts the descriptors for all pixels are calculated.
For the Fourier-Mellin calculation the images are extended by mirroring the edges to have a uniform-sized, 5x5, descriptor in all cases.
For LBP features two different settings are considered, one with a radius of 3 and 12 sample points and one with a radius of 5 and 24 sample points.
After that similar descriptors are merged and their frequency is calculated.
If the histogram correlation of two LBP features is higher than 90\%, if the distance between two SIFT features is lower than 300 and if the correlation between two Fourier-Mellin features is higher than 1.4 the two descriptors are combined.
Afterwards the most frequent noise descriptors are determined, occurring at least 100 times among LBP and at least 10 times among SIFT features.
All LBP and SIFT descriptors are eliminated which have higher than 90\% correlation with or have smaller than 250 distance to any dominant noise descriptor.
The remaining features are then propagated to the next layer.
In case of Fourier-Mellin Transform all descriptors are selected which occur at least 10 times on the given pattern.
\par
In the second layer the dominant descriptors of multiple samples are compared.
If there is a feature in every training image with a high enough similarity the feature is chosen and added to the final descriptor pool.
If the correlation between two LBP descriptor is higher than 90\%, the distance between two SIFT features is smaller than 450 and the correlation between two Fourier-Mellin features is higher than 1.4, the feature is chosen and added to the final set of descriptors.
At the end of this process the whole set of descriptor is written into file.
\par
If the learned descriptors are available the input image is processed.
The descriptors of the input image are determined similarly as in the training phase.
For every pixel of the input is a descriptor calculated, in case of Fourier-Mellin transform the borders of the input are extended by mirroring.
The output image is calculated by comparing the descriptors of the input with the ones in the descriptor pool.
In case of LBP and Fourier-Mellin features the correlation value is written into the resulting image which is normalized at the end of calculation.
The result image of the SIFT feature comparison is binary, it is set to one if the descriptor of the corresponding pixel has a smaller distance than 200 to any descriptor from the learned set.
At the end of the application the resulting images are saved.

\section{Evaluation}
\par
In this section the results of the modified three-layered learning algorithm are presented and discussed.
Furthermore it is examined, if this approach is usable for real shoeprint identification algorithms.
\par
Two kinds of experiments were conducted to test the performance of the learned descriptors.
First images from training data are processed to see how many descriptors from the original image were eliminated and to identify the common features in the training dataset.
After that new images are set as input to examine if the features learned on one kind of data are able to describe other kinds of shoeprint as well.
Figure \ref{fig:pe:25} and Figure \ref{fig:pe:66} are example images from the first scenario, since Figure \ref{fig:pe:25:orig} and Figure \ref{fig:pe:66:orig} are part of the training set.
The visible difference between Figure \ref{fig:pe:25:LBPs} and Figure \ref{fig:pe:25:LBPb} as well as between Figure \ref{fig:pe:66:LBPs} and Figure \ref{fig:pe:66:LBPb} shows that bigger radius is a better choice for shoeprint description.
Considering the middle region of Figure \ref{fig:pe:25}, the background on Figure \ref{fig:pe:25:LBPb} is less noisy than on Figure \ref{fig:pe:25:LBPs}.
This difference is more outstanding on  Figure \ref{fig:pe:66} where on Figure \ref{fig:pe:66:LBPs} no shoeprint can be recognized meanwhile on Figure \ref{fig:pe:66:LBPb} outlines of the pattern are visible.
Still focusing on the middle area of Figure \ref{fig:pe:25}, Figure \ref{fig:pe:25:SIFT} shows that the SIFT descriptor is similarly robust against noise as the LBP descriptor.
However, examining Figure  \ref{fig:pe:25:FM} the Fourier-Mellin descriptor has more difficulties with the background area, where the middle regions is less homogen than on  Figure \ref{fig:pe:25:LBPb} and on Figure \ref{fig:pe:25:SIFT}.
On the other hand, inspecting the bottom left side of the shoeprint, it is seen that the Fourier-Mellin features menaged to find the whole area of pattern, whereas only a few outlines are recognizable on Figure \ref{fig:pe:25:LBPb} and on Figure \ref{fig:pe:25:SIFT}.
Interestingly that said region is more recognizable on  \ref{fig:pe:66:LBPs} than on  \ref{fig:pe:66:LBPb}.
Based on these observations it is stated that the features less robust against noise are able to find a higher portion of pattern area than those with higher robustness.
In exchange the background area stays noisy and further image processing is needed.
Comparing all results of the two images, Figure \ref{fig:pe:25} and Figure \ref{fig:pe:66}, the performance strongly depends on the input quality.
On Figure \ref{fig:pe:66} and Figure \ref{fig:pe:66:SIFT} the pattern is less recognizable than on the previous example and a higher amount of background pixels are labeled wrongly as pattern.
On cluttered images Fourier-Mellin features seem to outperform both SIFT and LBP descriptors.
Overall this testing scenario shows ambivalent results.
On the one hand on clear samples LBP and SIFT is able to distinguish between fore- and background correctly, on the other hand, if the input data is cluttered those two descriptors are barely able to recognize the pattern and the Fourier-Mellin descriptor provides the clearest results. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00025.jpg}
    \subcaption{Example image from the training set}
    \label{fig:pe:25:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00025.jpg}
    \subcaption{Output using LBP descriptors with radius 3 and 12 sample points}
    \label{fig:pe:25:LBPs}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00025_6_24_5.jpg}
    \subcaption{Output using LBP descriptors with radius 5 and 24 sample points}
    \label{fig:pe:25:LBPb}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/FM/00025.jpg}
    \subcaption{Output using Fourier-Mellin Descriptor}
    \label{fig:pe:25:FM}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/SIFT/00025.jpg}
    \subcaption{Output using SIFT Descriptor}
    \label{fig:pe:25:SIFT}
  \end{subfigure}
  \caption{Output of the modified three-layered learning algorithm on an image from training set}
  \label{fig:pe:25}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00066.jpg}
    \subcaption{Example image from the training set}
    \label{fig:pe:66:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00066_4_12_3.jpg}
    \subcaption{Output using LBP descriptors with radius 3 and 12 sample points}
    \label{fig:pe:66:LBPs}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00066_6_24_5.jpg}
    \subcaption{Output using LBP descriptors with radius 5 and 24 sample points}
    \label{fig:pe:66:LBPb}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/FM/00066.jpg}
    \subcaption{Output using Fourier-Mellin Descriptor}
    \label{fig:pe:66:FM}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/SIFT/00066.jpg}
    \subcaption{Output using SIFT Descriptor}
    \label{fig:pe:66:SIFT}
  \end{subfigure}
  \caption{Output of the modified three-layered learning algorithm on an image from training set}
  \label{fig:pe:66}
\end{figure}

\par
On Figure \ref{fig:pe:182} the results of the modified three-layered learning algorithm on a non-training sample are shown.
The results displayed on Figure \ref{fig:pe:182:LBPs} and on Figure \ref{fig:pe:182:LBPb} strengthens the observation that bigger neighborhood LBP outperforms the smaller radius descriptor.
Furthermore examining the middle, noisy part of the images in all three cases a homogenous background is seen containing some falsely labelled pixels.
Observing the bottom part of the shoe a weakness of LBP and SIFT already discussed is noticeable.
On the original image there are small structures which are part of the shoeprint.
Similarly as on the bottom left part of Figure \ref{fig:pe:25} the fine structures are only partially recognized, comparing Figure \ref{fig:pe:182:FM} to Figure \ref{fig:pe:182:LBPb} and to Figure \ref{fig:pe:182:SIFT} Fourier-Mellin outperforms LBP and SIFT again labelling the whole area correctly.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/00182.jpg}
    \subcaption{Input image}
    \label{fig:pe:182:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00182_4_12_3.jpg}
    \subcaption{Output using LBP descriptors with radius 3 and 12 sample points}
    \label{fig:pe:182:LBPs}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/LBP/output_00182_6_24_5.jpg}
    \subcaption{Output using LBP descriptors with radius 5 and 24 sample points}
    \label{fig:pe:182:LBPb}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/FM/00182.jpg}
    \subcaption{Output using Fourier-Mellin Descriptor}
    \label{fig:pe:182:FM}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{3/SIFT/00182.jpg}
    \subcaption{Output using SIFT Descriptor}
    \label{fig:pe:182:SIFT}
  \end{subfigure}
  \caption{Output of the modified three-layered learning algorithm}
  \label{fig:pe:182}
\end{figure}

\par
To summarize LBP and SIFT features are less responsive in the noisy area and have difficulties to find fine patterns while the Fourier-Mellin transform labels bigger parts of the shoeprint correctly whereas it more often mistakes the noise as foreground.
Based on the testing images Fourier-Mellin recognize the shoeprint pixels in noisy images better than LBP and SIFT.
A possible solution is to combine all three feature descriptors and propose a weighting score across them, assuming that all three feature sets were able to provide usable results for further processing.
However, in low quality images that is not necessarily the case.
Even though Figure \ref{fig:pe:66} was part of the testing dataset, LBP and SIFT did not label the pattern correctly.
That problem can originate from the learning process, where all dominant descriptors of the noise part are eliminated from the pattern descriptors.
On the other hand on Figure \ref{fig:pe:66:SIFT} significant amount of background is labelled as pattern, which indicates that too many noise descriptors were selected into the descriptors pool.
Furthermore there are several lower quality samples in the whole dataset available then the one presented on Figure  \ref{fig:pe:66}.
It has to be emphasized that the descriptors were learned on a small dataset.
Incorporating more samples and altering the learning criteria, e.g. the given feature has to be found in a given ratio of training images and not on every one of them, can lead to a broader texture pool.
However, this also results in a higher ratio of noise descriptors in the final feature set.
High noise ratio is a limitation of this algorithm in its current form.
For that reason noise suppression techniques are examined in the following.
In the future the modified three-layered learning algorithm can be further developed and used jointly with noise suppression techniques to solve its weakness and to take advantage of its results.

\chapter{Fully-Automated Noise Elimination}

Since the algorithm introduced in the previous chapter had difficulties when processing noisy or cluttered data a noise suppression technique is introduced now, a fully-automated in the following one a semi-automated approach is proposed.
Similar to the previous method the main goal is to distinguish between noise and pattern, but this time instead of finding pattern regardless of the noise, the noise is suppressed first and the information is enhanced afterwards.
\par
To compare the proposed methods more easily, the structure of this chapter is the same as the previous one.
The methodology is discussed first, afterwards implementation details are given, finally the results are examined and evaluation of the results is discussed.

\section{Methodology}
\par
The method proposed in this chapter has three main steps, first the noise is suppressed, after that the shoeprint pattern is enhanced and the result image is generated lastly by thresholding.
The workflow of the entire algorithm is shown on Figure \ref{fig:fans:workflow}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{4/flow.jpg}
  \caption{The workflow of the fully automated noise suppression algorithm.}
  \label{fig:fans:workflow} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
The first part of the algorithm is responsible for noise suppression.
Since there is little research done on enhancement of real-life forensic images two filters proven to be effective in natural image noise elimination are combined.
As Li et al. \cite{li2014rapid} state, Wiener filter is among the most popular techniques in the topic of noise reduction, however it does not reconstruct the signal data it only suppresses the noise component.
Wiener filter works in the signal domain, where it estimates the original image based on the cluttered input. 
The filter is given by following Equation \cite{Win}:
\[ G(u, v) = \frac{H^*(u,v) P_s(u, v)}{\mid H(u,v)\mid ^2 P_s (u, v) + P_n (u, v)}  \]
where $H(u,v)$ stands for the Fourier Transform of the point-spread function, $P_s (u,v)$ denotes the power transform of the signal, which is the Fourier Transform of the signal autocorrelation and $P_n(u,v)$ expresses the power spectrum of noise, which is the Fourier Transform of the noise autocorrelation.
The performance of the filter depends on the quality of $P_s$, on the estimated appearance of the original image.
In this method the estimation is made based on the input image.
The other technique used for noise suppression is Bilateral Filtering as proposed in \cite{huang2013self} and in \cite{zhang2016simultaneous}.
Similar to the Wiener Filter Bilateral Filter is also used for smoothing the image, since it considers the weighted average of neighboring pixels, it preserves the edge information which is crucial to prevent blurring on the shoeprint pattern. 
When both images are calculated the difference between them is propagated.
In this way the blurring effect is strengthened in the background while the outlines of the shoeprint patterns are preserved.
\par
For enhancement Successive Mean Quantization Transform (SMQT) \cite{nilsson2013smqt} is used as proposed by Katireddy et al. \cite{katireddy2017novel}.
SMQT is recursive algorithm which splits the data into two parts in every recursion level depending on the current data value being smaller or bigger than the mean of the given data part.
In this way the structure of the data is exposed.
Everytime the image is separated into two parts, it is noted first which pixel is under (0) and which one is above (1) the current mean.
The algorithm then recursively continues on both subgroups of the images, splitting and noting the relative pixel value again.
Note that the pixels in the same subgroup does not have to be neighboring, the clustering is solely based on the illumination value.
The recursion stops if the predefined depth is achieved.
To finish the transformation the noted cluster values of the pixels are examined.
Along the levels of the recursion a sequence of ones and zeros are registered for every pixel.
As a last step this sequence is considered as a binary number and after converting into decimal value it is updated as the intensity of the corresponding pixel.
\par
The last part of the application is the postprocessing step where the input is converted into a binary image and small inconsistencies and remaining noise are eliminated.
When binarizing footprint images Otsu's technique \cite{algarni2008novel}, \cite{alizadeh2017automatic}, \cite{wu2019crime} or adaptive thresholding \cite{wang2014automatic} is used most frequently.
However in the proposed enhancement approach Niblack Binarization Method (NBM) \cite{niblack1985introduction} is preferred.
NBM is a local thresholding method and it is calculated as follows \cite{saxena2019niblack}:
\[T_d = m(x,y) + k * s(k, y)\]
where $m$ and $s$ stands for mean and standard deviation in the given area respectively and $k$ is a configuration variable which is given manually. 
There are several publications available \cite{som2011application}, \cite{athimethphat2011review}, which prove that global methods, such as Otsu Thresholding \cite{otsu1979threshold}, is less feasible as their local counterparts.
Although the studies mentioned were carried out on text documents with varying image quality, the two domains are considered as familiar since they both aim to find fine line structures on a cluttered background.
Furthermore, Saxena et al. \cite{saxena2019niblack} also state that NBM is one of the most powerful thresholding methods, outperforming the global and some local techniques as well. 
There is however one disadvantage of local binarization and that is local window size.
Since for every subwindow a new threshold is calculated NBM tends to generate salt and pepper noise on more homogeneous, e.g. background, area.
For that reason two other postprocessing methods are also implemented.
The first one eliminates short lines on the image, based on the assumption that the actual shoe pattern outlines are recognized and they build bigger, coherent edges.
The second algorithm stands on the same principle as well, from the remaining lines are all open structures eliminated unless the length is higher than a given threshold bigger than the one in the previous step.
It is based on two observations, first if the complete structure of a pattern element is found a closed line structure is extracted, since NBM concentrates on the contours.
Second, when there is no pattern element in a NBM subwindow only the edges of the remaining clutter and binarization artifacts are generated.
Since those lines are most likely not closed, they are deleted.
However it is still possible, that an open line structure is generated if only parts of the shoeprint are classified correctly.
For that reason, not every open edge, only a subgroup of them, which are shorter than a threshold, is eliminated.

\section{Implementation}
\par
Implementation details and parameter settings are discussed  in the following.
The programming tools are similar to the ones in the previous chapter, Python 2.7 \cite{van1995python} with OpenCV 4.1.1 \cite{opencv_library} were used for development.
Since all the methods presented in this thesis are prototypical applications, the parameter settings were determined experimentally.
\par
At the start of the application the input image is read and forwarded to the denoising stage.
The original image is processed separately with Wiener and Bilateral Filter.
To apply the Wiener Filter correctly the input image is normalized first to the range of 0 to 1.
For the Point Spread Function an image of size 5x5 is set.
The balance parameter is 1100 which sets the ratio between information adequacy and prior adequacy, those parameters control frequency increment and decrement respectively.
The kernel size of the Bilateral Filter is 9x9.
When both filtered images are calculated their difference is determined and propagated to the enhancement step of the algorithm.
\par
For SMQT calculation, because of time efficiency , a speeded up version is implemented.
In the first step a table of occuring values on the image is created where the zeroth column corresponds the pixel value of 0, the first column stands for pixel value of 1 and so on.
In the first row of the table, the frequency of the given value is noted, that is the number of pixels having the value which the column corresponds with.
In the second row the sum of frequencies up to the current column is stored.
The third row represents the sum of all elements until the given column.
This table eases the calculation of mean value for every level of the SMQT algorithm.
Additionally, since the entries are ordered, if the subgrouping starts, the values belonging to the same cluster will be neighboring columns.
Figure \ref{fig:fans:table} shows the first 14 columns of the occurrence map of an example image.
When the occurrence map is ready the recursion starts, the mean of the given subgroup is determined and split into two parts according to the pixel value of the given column being bigger or smaller than the mean value. 
Every column of the table have a binary code which is created during the recursion.
In the first level the first digit of the code is written, in the second level the second digit etc. the length of this code is the same as the predefined depth of the recursion.
If a pixel value is smaller or equal to the calculated mean of the given cluster a zero is written in the belonging binary code, and a 1 is recorded if it is bigger.
The manually set depth of the recursion is 8, in this way when the final binary codes are converted to a decimal number a range of 0 to 255 is covered.
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.09\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.9\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/table.jpg}
  \end{subfigure}
  \caption{The first 14 columns of the occurrence map. The header of the table represents the possible pixel values }
  \label{fig:fans:table} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}
\par
In the last step the image is thresholded according to NBM.
The window size is the same as the size of the Wiener Filter, 5x5, the regularization parameter $k$ of NBM is set to 0.5.
After that short and not-long open edges are eliminated.
The connected components of the image are extracted first considering 8-neighborhood.
Afterwards every component is examined on their size, if they are smaller than a given threshold they are deleted.
It is possible to set the threshold to the average size of every region, however, experimental results show that such a high criterion eliminate shoe pattern lines as well.
For that reason a manually set threshold is used, which is set to 50 for every test image.
The short line elimination algorithm is applied on both the fore- and on the background on the image.
Lastly the remaining components are examined on their openness.
If an open line structure is found which overall length is smaller than a manually set threshold it is deleted.
This threshold is set to be 60 during the whole experiment.

\section{Evaluation}
\par
In the following section the experimental results are presented.
Furthermore, the performance and the usability of the proposed algorithm is discussed.
Figures \ref{fig:fans:denoise} and \ref{fig:fans:enhance} show the output of the proposed algorithm on every stage and present the final results as well.
\par
Figure \ref{fig:fans:denoise} shows the output of the denoising pipeline on three example images from the FID-300 database.
In the second column the Wiener filtered images are shown, in the third one the Bilateral filtered images are displayed whereas the difference between those two pictures is in the last row.
Considering the background of the algorithms and the amount of noise on the images a light smoothing is done in the denoising phase.
This decision was made to protect the valuable pattern information.
Since no previous knowledge is available about the properties of shoe print, no aggressive filter kernel is used to make sure that the relevant information is preserved correctly.
On the combined image a bright silhouette around the shoeprint image is visible, which was created by the Wiener Filter.
This feature is exploited in a later stage of the algorithm, where an edge sensitive binarization method is used.

\begin{figure}[h]

\subfloat{
  \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241.jpg}
    \subcaption{Example image}
    \label{fig:fans:241:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_wiener.jpg}
    \subcaption{Wiener filtered image}
    \label{fig:fans:241:wiener}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_bi5.jpg}
    \subcaption{Bilateral filtered image}
    \label{fig:fans:241:bi}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_denoise.jpg}
    \subcaption{Denoised image}
    \label{fig:fans:241:denoise}
  \end{subfigure}
  \label{}
}

\subfloat{
    \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235.jpg}
    \subcaption{Example image}
    \label{fig:fans:235:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_wiener.jpg}
    \subcaption{Wiener filtered image}
    \label{fig:fans:235:wiener}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_bi5.jpg}
    \subcaption{Bilateral filtered image}
    \label{fig:fans:235:bi}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_denoise.jpg}
    \subcaption{Denoised image}
    \label{fig:fans:235:denoise}
  \end{subfigure}
}

\subfloat{
  \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021.jpg}
    \subcaption{Example image}
    \label{fig:fans:21:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_wiener.jpg}
    \subcaption{Wiener filtered image}
    \label{fig:fans:21:wiener}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_bi5.jpg}
    \subcaption{Bilateral filtered image}
    \label{fig:fans:21:bi}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_denoise.jpg}
    \subcaption{Denoised image}
    \label{fig:fans:21:denoise}
  \end{subfigure}
}

\caption{Example for the results of the denoising stage}
\label{fig:fans:denoise}

\end{figure}

\par
The second step of the algorithm is enhancement where SMQT is applied, and the results are shown on Figures \ref{fig:fans:241:enhance}, \ref{fig:fans:235:enhance} and \ref{fig:fans:21:enhance}.
The benefit is the SMQT algorithm is ambiguous.
On the one hand focusing on the middle area of Figures  \ref{fig:fans:241:enhance} and \ref{fig:fans:235:enhance} the difference between fore- and background is bigger than on the original image.
The contours of the shoeprint are more outstanding than on the original image.
Furthermore the halo effect is also more visible than on the output of the previous stage of the algorithm.
Comparing Figure \ref{fig:fans:235:enhance} to Figure \ref{fig:fans:21:denoise}, the shoe pattern structures got darker and there is a brighter area around the contours than on Figure \ref{fig:fans:21:denoise}.
On the other hand not only the information but also the remaining noise elements got enhanced by SMQT.
Even though the denoising stage is finished, since only moderate smoothing was applied, there is a considerable noise on the input images for SMQT.
The soft filtering is able to cope with Gaussian noise like on Figure \ref{fig:fans:241:denoise} but it does not eliminate bigger clutter particles like on the left side of Figure \ref{fig:fans:235:denoise} or on the upper region of Figure \ref{fig:fans:21:denoise}.
The SMQT enhancement makes those clutter more obvious by increasing the color difference between the noise and the homogenous background, for example the already mentioned patches on the left side of Figure \ref{fig:fans:235:denoise} are now more visible than on the original image \ref{fig:fans:235:orig}.
Additionally SMQT has similar effect like Histogram Equalization, the background on Figure \ref{fig:fans:241:denoise} is near homogeneous, but after applying SMQT its color changes are more obvious since the original pixel values are mapped now to a wider color range.

\begin{figure}[h]

\subfloat{
  \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241.jpg}
    \subcaption{Example image}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_enhnace.jpg}
    \subcaption{Enhanced image}
    \label{fig:fans:241:enhance}
  \end{subfigure}
 \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_threshold.jpg}
    \subcaption{Binarized image}
    \label{fig:fans:241:bin}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/241/00241_result.jpg}
    \subcaption{Output image}
    \label{fig:fans:241:out}
  \end{subfigure}
}

\subfloat{
    \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235.jpg}
    \subcaption{Example image}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_enhnace.jpg}
    \subcaption{Enhanced image}
    \label{fig:fans:235:enhance}
  \end{subfigure}
 \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_threshold.jpg}
    \subcaption{Binarized image}
    \label{fig:fans:235:bin}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/235/00235_result.jpg}
    \subcaption{Output image}
    \label{fig:fans:235:out}
  \end{subfigure}
}

\subfloat{
  \centering
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021.jpg}
    \subcaption{Example image}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_enhnace.jpg}
    \subcaption{Enhanced image}
    \label{fig:fans:21:enhance}
  \end{subfigure}
 \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_threshold.jpg}
    \subcaption{Binarized image}
    \label{fig:fans:21:bin}
  \end{subfigure}
  \begin{subfigure}[b]{0.24\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{4/21/00021_result.jpg}
    \subcaption{Output image}
    \label{fig:fans:21:out}
  \end{subfigure}
}

\caption{Example for the results of the enhancement and postprocessing stage}
\label{fig:fans:enhance}

\end{figure}

\par
The closing step of the algorithm is postprocessing, consisting of binarization (the third column of Figure \ref{fig:fans:enhance}) and line-clutter elimination (in the fourth column of Figure \ref{fig:fans:enhance}).
As expected NBM is able to take advantage of the halo effect and finds the shoe pattern correctly. 
On all three images, Figure \ref{fig:fans:241:bin}, Figure \ref{fig:fans:235:bin} and Figure \ref{fig:fans:21:out}, the contours of the patterns are clearly visible.
However, the drawbacks of NBM and the enhanced noise by SMQT are evident, since among the correct outlines a high amount of noise is generated.
For that reason the closing step of short line and open line structure elimination is executed.
On Figures \ref{fig:fans:241:out} and \ref{fig:fans:235:out} the majority of background is cleared up and the shoeprint pattern is shown correctly.
On the other hand on Figure \ref{fig:fans:21:out} the original pattern is not recognizable anymore, along with the background noise relevant contour edges were also eliminated, and noise lines attached to the shoeprint pattern were kept.
\par
The presented results of the fully-automated noise elimination algorithm indicate two things.
First, NBM with the current postprocessing technique works better on samples with fine lines, Figure \ref{fig:fans:241:orig}, and with detailed small structures, Figure \ref{fig:fans:235:orig}.
As already discussed and shown NBM tends to generate line noise on homogenous areas, if the shoeprint consist of dense pattern, there is no big, homogenous background between the different structure elements thus no noise is generated.
Additionally the heuristics which the two postprocessing steps are based on do not apply on every sample on the database.
On Figure  \ref{fig:fans:21:bin} even though there is a high amount of noise the pattern is clearly recognizable whereas on Figure \ref{fig:fans:21:out} no shoeprint can be seen anymore.
This implies that the settings of the line elimination algorithms are not correct and that further postprocessing is needed
Several pattern edges were eliminated because they were too short.
The threshold for line length were not changed during testing, thus a possible solution for the problem is to always adjust the parameter according to the current image.
This adjustment is however difficult since there is no a-priori knowledge about the sample available.
Alternatively looking to Figure  \ref{fig:fans:21:out} the outlines of the pattern were found originally, but because of the noise they were not connected.
In the application 8-neighborhood is considered, a more allowing connectivity criteria, such as neighborhood in a given range, leads closed contour edges.
But this causes more connectivity among noise as well.
Furthermore it also magnifies the second problem occurring on Figure \ref{fig:fans:21:out}, that noise lines are connected to the contours, thus are not eliminated.
An additional postprocessing step is needed to examine the remaining edges and determine which connection was made unintentionally.
\par
Second, Figures \ref{fig:fans:235:out} and \ref{fig:fans:21:out} show that the performance of fully-automated noise elimination algorithm strongly depends on the quality of the input image.
This algorithm has a similar limitation like the modified three-layered learning approach proposed in the previous chapter.
Initially the filter settings of the denoising step are not able to handle cluttered noise seen on Figures \ref{fig:fans:235:orig} and \ref{fig:fans:21:orig}.
Chatterjee et al. \cite{chatterjee2011patch} proposed a method which represents the estimated image for Wiener Filter based on geometrically and photometrically similar regions.
Even though their approach had promising results, it was tested on data with Gaussian noise and not with irregular cluttered noise seen on the majority if FID-300 images.
Later, in the enhancement step not only the information but the remaining noise elements are also enhanced.
Lastly, during binarization an extensive amount of noise is generated which is  not eliminated perfectly afterwards.
Saxena et al. \cite{saxena2019niblack} analyse several improvements for NBM to make it more robust and performant in poor conditions.
But based on the experiments presented in this section image quality and high noise ratio is a major limitation of the method.

\par
Furthermore, the test images showed in this chapter are high quality samples compared to other members of the FID-300 database.
The evaluation of the modified three-layered feature learning algorithm and the fully-automated noise elimination method show that these approaches are not able to overcome the limitation of the high amount of varying clutter on the sample images.
The second algorithm showed promising results in ideal and very low quality in poor conditions, whereas the first one delivered average results regardless the initial properties of the input image.
Furthermore, it also has to be mentioned that both algorithms were tested on high or middle quality samples, images with poor shoeprints or high noise ratio like on Figure \ref{fig:rw:lowFID} were excluded during evaluation.
To be able to process those kind of images as well, additional knowledge about the input is needed.
For that reason a semi-automated algorithm is introduced next.
Since the modified three-layered feature learning algorithm is able to make an approximate estimation about the location of pattern, information about the noise is collected to successfully enhance the shoeprint impressions.

\chapter{Semi-Automated Shoeprint Enhancement}
\par
The prototypical algorithms presented and evaluated in the previous two chapters have shown that high noise ratio and the versatile appearance of clutter is a crucial difficulty in real-life shoeprint enhancement.
The experiments also indicate that without previous knowledge about either the shoeprint or the noise on the sample it is difficult to distinguish between them.
Since noise tends to appear both on the fore- as well as on the background region, semi-automated algorithm is introduced in this chapter, where user input about the noise is expected.
\par
The structure of this chapter is similar to the two previous ones.
First the theoretical background and overall design of the proposed method is presented, after that an overview about the implementation is given and finally the results are evaluated briefly.

\section{Methodology}
\par
The algorithm proposed in this chapter has two main steps, first the background and foreground is separated, after that the foreground is enhanced.
There is one precondition when starting the application, the user has to select a pixel in the background area where no pattern information, so foreground, is in the neighboring area.
The full pipeline is shown on Figure \ref{fig:sans:workflow}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{5/flow.jpg}
  \caption{The workflow of the semi-automated noise suppression algorithm.}
  \label{fig:sans:workflow} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
Based on the user input the background area is determined according to the Fourier-Mellin features \cite{sheng1986circular}.
Fourier-Mellin features are scale, rotation and translation invariant descriptors and are calculated according to the following Equation \cite{kazik2011visual}:
\[\mathcal{M}_f(u,v) = \frac{1}{2\pi} \int_{0}^{\infty}\int_{0}^{2\pi} f(r, \theta)r^{-ju}e^{-jv\theta}\mathrm{d}\theta\frac{\mathrm{d}r}{r}\]
where $u$ and $v$ are the Mellin and the Fourier transform parameter respectively.
If the same image is translated the Fourier-Mellin transformed representation does not change, that is however not the case when the image is scaled or rotated.
In both scenarios phase shift appears on the feature descriptor and the magnitude changes according to the scaling factor.
Because of that the feature descriptor is converted to Log-Polar coordinates, so that said image transformations occur as simple translation in the descriptor.
Log-Polar Transform is used to achieve robustness against scale and rotation.
The Log-Polar coordinates of any point with Cartesian coordinates are given by \cite{sarvaiya2012image}:
\[(\rho,\theta) = (\sqrt{(x-x_c)^2 - (y-y_c)^2}, \tan^{-1}\frac{y-y_c}{x-x_c})\]
where $x$ and $y$ are the Cartesian coordinates of the point and $x_c$ and $y_c$ are the center coordinates of the input image.
The Log-Polar coordinates represent the radial distance, $\rho$, and the angle from the center, $\theta$.
This conversion is illustrated on Figure \ref{fig:sans:logPol} as well.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{5/logPol.jpg}
  \caption{Illustration of the relation between Log-Polar and Cartesian coordinates \cite{sarvaiya2012image}}
  \label{fig:sans:logPol} % \label has to be placed AFTER \caption (or \subcaption) to produce correct cross-references.
\end{figure}

\par
Fourier-Mellin features were already used for shoeprint description in both synthetic \cite{gueham2008automatic} and real \cite{wu2019crime} datasets, however in this algorithm it is used for noise description.
This decision is based on the observation, that although the noise varies among samples, there are dominant features within one image which describe the majority of the clutter, on a large part of the testing images.
Even though those dominant features are determined, noise has an irregular structure by definition as well.
To prepare the descriptor for that variance three-different sized samples are extracted around the point determined by user input.
The Fourier-Mellin descriptors of the three subimages represent the noise, after that every pixel of the input is examined for correspondence with the same metric defined in Chapter 3 \ref{FMcorr}.
The calculated similarities are stored in a correlation map which is binarized by Otsu's method afterwards.
Otsu thresholding is proven to be effective in all three kinds of data, synthetic \cite{algarni2008novel}, \cite{alizadeh2017automatic}, restricted \cite{kong2014novel} and also on real forensic datasets \cite{wu2019crime}.
To complete the background mask morphological operations are used to eliminate small holes and rough contours as proposed in many approaches for shoeprint identification \cite{wang2014automatic}, \cite{kong2014novel}, \cite{li2014retrieval}, \cite{tang2010footwear}.
\par
After the background mask is generated the foreground of the image is enhanced in three main steps.
Noise on the shoeprint pattern is suppressed first, afterwards the edge information is extracted and the binary images is generated lastly.
Wavelet transform is often used for natural image denoising \cite{xu2016image}, \cite{sugamya2016image}, for fingerprint \cite{li2012texture} and for shoeprint impression enhancement \cite{katireddy2017novel} as well.
During wavelet transform the image is decomposed into subbands, and the subimages are processed separately.
Assuming four subbands, the LL image contains the edge information and the other ones the intensity values.
Depending on the properties of noise the image clutter is separated from the information and suppressed on the corresponding sub-band.
After that the image is reconstructed by inverting the decomposition process.
The same principle is used in this approach but instead of wavelet transform simple Fourier transform is applied.
When the background is separated a model about the appearance of the noise is built.
Processing the foreground area and the created noise model with Discrete Fourier Transform, a signal domain representation  of both images is created.
The noise is eliminated by subtracting the noise frequencies from the foreground frequencies and inverting the resulting image with the Inverse Fourier Transform.
\par
For line extraction a gradient based method published by Zeng et al. \cite{zeng2011region} is used.
They propose a region based non-local means algorithm to denoise natural images.
The pixels are clustered according to the eigenvalues of the tensor matrix given by \cite{zeng2011region}:
\[T_\sigma = 
\begin{pmatrix}
t_{11} & t_{12} \\
t_{12} & t_{22}
\end{pmatrix}
=
\begin{pmatrix}
G_\sigma*(g_x(i,j))^2 & G_{\sigma}*g_x(i, j)g_y(i, j)\\
G_{\sigma}*g_y(i, j)g_x(i, j) & G_{\sigma}*(g_y(i, j))^2
\end{pmatrix}
\]

where $g_x$ and $g_y$ represent the gradient information in the corresponding directions and $G_\sigma$ is the Gaussian kernel having $\sigma$ as standard deviation.
The eigenvalues of $T_\sigma$ are then given by \cite{zeng2011region}:
\[\lambda_1 = \frac{1}{2}(t_{11} + t_{22} + \sqrt{(t_{11}-t_{22})^2 + 4t_{12}^2})\]  
\[\lambda_2 = \frac{1}{2}(t_{11} + t_{22} - \sqrt{(t_{11}-t_{22})^2 + 4t_{12}^2})\]  
\label{eig}
The classification is based on the difference between the two eigenvalues of the given pixel.
If the difference is small it indicates smooth region, whereas a higher value implies edge area.
The proposed classification strategy is the following \cite{zeng2011region}:
\[
(i, j)\in \left\{
                \begin{array}{ll}
                  c_1, if \lambda(i,j) \leq \lambda_{min} + \frac{1(\lambda_{max} - \lambda_{min})}{n}\\
                  c_2, if \lambda(i,j) \leq \lambda_{min} + \frac{2(\lambda_{max} - \lambda_{min})}{n}\\
				... \\
                   c_n, if \lambda(i,j) \leq \lambda_{min} + \frac{n(\lambda_{max} - \lambda_{min})}{n}
                \end{array}
              \right.
\]

where $\lambda_{min}$ and $\lambda_{max}$ stays for the minimum and maximum difference on the entire image.
For denoising the mean illumination value of pixels in the same class is calculated and the original pixel value is replaced with the average of the cluster.
Zeng et al. \cite{zeng2011region} propose an additional weighting scheme as well, so that pixels in the same class with smaller geometrical distance have a higher weight than those further away on the image while determining the mean.
The region based non-local means algorithm was originally developed for noise removal on natural images but it is also applicable for line extraction on shoeprint impressions.
Since the classification is made based on the gradients, edge properties are the essential clustering criteria.
Assuming that both the shoeprint pattern and the remaining noise on the image have common properties which are different to each other, pattern and noise classes are created at the end of the custering.
However, the weighting method is skipped in this implementation, geometrical location is not relevant in this use-case for two reasons.
First the size of the region the image was taken of is smaller in general than in case of natural images.
Second, the shoeprint impressions are 2 Dimensional images in their original form as well. 
Since there is no depth difference within the image, the entire area is considered as one connected region unlike on natural images where the objects in the foreground and the background do not belong together.
On the other hand another criteria is added to the clustering algorithm.
After determining the classes if one or more groups have more members than a given threshold, histogram equalization is applied on the image where the differences of gradients are stored and the classes are recalculated.
This optimization is needed to make the classes representative and to lower the chance, that pattern and noise edges are assigned to the same cluster. 
\par
To find  the cluster with the noise edges a simple heuristic is defined.
Since the line clustering algorithm is applied on the entire image, not only on the foreground, the class with the highest amount of members is eliminated.
This decision is based on the assumption that noise is found on the entire image, whereas pattern edges are located only on given areas.
Furthermore, it is also assumed that unlike shoeprint contours, where the edge properties change according to the original shoeprint structures and pressure, noise have similar appearance on the entire sample.
After that the image is binarized with the adaptive thresholding technique \cite{laine1996multiscale} popular in both fields of shoeprint recognition  \cite{wang2014automatic}, \cite{li2014retrieval} and also of natural image processing \cite{xu2016image}.
Adaptive thresholding is a local binarization technique similar to NBM, generating multiple thresholds for every subregion of an image.
\par
The closing step is postprocessing, where small discontinuities and remaining noise are eliminated.
Despite in the previous step noise edges belonging to the biggest class were eliminated, other noise lines assigned to different classes are still part of the image.
The problems is solved with short line elimination.
Remaining noise elimination while preserving the shoe pattern is possible because of the assumption that noise consists of cluttered small components whereas the shoeprint is built of bigger, connected structures.

\section{Implementation}
\par
In the following implementation details of the proposed algorithm are given.
Identical to the implementation of the previously discussed methods Python 2.7 \cite{van1995python} with OpenCV 4.1.1 \cite{opencv_library} was used.
Because of the prototypical implementation, the parameter settings were determined experimentally.
\par
Starting up the application the user is required to select a point on the input, which neighborhood is representative for the dominant noise on the given image.
Based on the information given by the user background and foreground of the sample are separated.
To do so the Fourier-Mellin Features of the selected neighborhood are compared to the neighborhoods of every other pixel on the image.
Three different sized neighborhoods, diameter of 6, 12 and 18, are obtained and the average correlation value is noted.
The borders of the image is extended by mirroring to ensure that the calculated Fourier-Mellin Features have uniform size for every pixel on the image.
The resulting correlation image is thresholded with Otsu's method.
Afterwards two morphological operations are applied using an 8x8 kernel, Opening first and Closing second.
The last step is normalization to the range of 0 to 1 to generate a binary image for background area.
\par
After determining the background area the foreground is enhanced.
The first step is to eliminate noise on the foreground according to the noise estimation made on the background area.
If the background area is smaller than 20\% of the image, this step is skipped, because in this case no robust model about the clutter can be made.
To estimate the average noise appearance the original image is extended again and the Fourier image of size 6x6 of every pixel is calculated.
To build a noise model two methods were used.
The first one is simple average calculation.
The second one represents noise by the centroids of the k-means algorithm.
For that double criteria is set, the calculation terminates if the wished accuracy is achieved, however that is set to 1.0, or stop after 10 iterations.
k is set to 8, so 8 centroids are determined, thus 8 submodels are used when eliminating the noise on the pattern.
In case of the averaged noise model, the Fourier image of noise is subtracted from the Fourier image of the foreground pixel and the image is reconstructed using the Inverse Fourier Transform.
When using the centroid noise models, the Fourier image of the foreground pixel is compared to all possible submodels, and the one with highest correlation is used.
The similarity is determined using the Equation for Fourier-Mellin Feature comparison \ref{FMcorr}.
When the centroid is decided identical to the approach of the other noise model, the difference between both Fourier images is calculated and the foreground is reconstructed using the Inverse Fourier Transform.
\par
When the shoeprint pattern is enhanced the edges of the entire image are enhanced.
First the differences between the eigenvalues are calculated for every pixel.
To do so the image is blurred first with the Gaussian kernel of size 11x11 to make the calculation more robust.
To extract the gradients the Sobel filter is used in x and y direction separately.
If the tensor matrix is calculated the eigenvalues are calculated as given in Equation \ref{eig}.
The differences are clustered as  Zeng et al. \cite{zeng2011region} suggest, two parameter settings are implemented using 25 and 20 classes respectively.
Afterwards it is examined if one cluster has more than 60\% of all pixels, in that case the classification is not representative and Histogram Equalization is applied on the gradient difference image and the memberships are recalculated.
Finally the non-local means are calculated for every subgroup and the pixels of every cluster are updated to the corresponding mean value.
\par
In the postprocessing step the cluster with the most members is eliminated first, when no Histogram Equalization was applied on the generated image.
In case of using histogram equalization the class memberships are more balanced, so no such heuristic is assumed.
After that the image is binarized with adaptive thresholding, where  the Gaussian weighted sum of the neighborhood is used, the window is 12 and constant is set to 2.
To prevent thresholding artifacts the background mask is applied after the binary conversion.
As the last step the small line structures are eliminated in both white and black channels.
The connected components of the image using 8-connectivity are extracted and every element which region not bigger than 15 is deleted immediately.
To be able to use the finally extracted and enhance shoeprint image later the resulting image is saved in file.

\section{Evaluation}
\par
In this section the experimental results are shown and the performance of the algorithm is discussed.
On Figures \ref{fig:sans:res1} and \ref{fig:sans:res2} example results are shown, the original samples are in the first column, the results using 20 classes for gradient clustering are shown in the second and third column, the output using 25 classes are in the fourth and fifth column.
Column two and four illustrate the results when the average noise model was used for pattern enhancement, whereas column three and five represent the output in case of using the centroids of the k-means algorithm.

\begin{figure}[h]

\subfloat{
  \centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/orig.jpg}
    \subcaption{																							Example image }
	\label{fig:sans:21}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/20_mean.jpg}
    \subcaption{Enhanced image using average noise model and 20 clusters for gradient classification}
    \label{fig:sans:21:20_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/20_cent.jpg}
    \subcaption{Enhanced image using centroids noise model and 20 clusters for gradient classification}
    \label{fig:sans:21:20_cent}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/25_mean.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:21:25_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/21/25_cent.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:21:25_cent}
  \end{subfigure}
}

\subfloat{
  \centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/orig.jpg}
    \subcaption{Example image}
	\label{fig:sans:25}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/20_mean.jpg}
    \subcaption{Enhanced image using average noise model and 20 clusters for gradient classification}
    \label{fig:sans:25:20_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/20_cent.jpg}
    \subcaption{Enhanced image using centroids noise model and 20 clusters for gradient classification}
    \label{fig:sans:25:20_cent}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/25_mean.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:25:25_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/25/25_cent.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:25:25_cent}
  \end{subfigure}
}

\caption{Example results of the semi-automated algorithm}
\label{fig:sans:res1}

\end{figure}

\begin{figure}[h]

\subfloat{
  \centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/orig.jpg}
    \subcaption{Example image}
	\label{fig:sans:174}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/20_mean.jpg}
    \subcaption{Enhanced image using average noise model and 20 clusters for gradient classification}
    \label{fig:sans:174:20_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/20_cent.jpg}
    \subcaption{Enhanced image using centroids noise model and 20 clusters for gradient classification}
    \label{fig:sans:174:20_cent}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/25_mean.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:174:25_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/174/25_cent.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:174:25_cent}
  \end{subfigure}
}


\subfloat{
  \centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/orig.jpg}
    \subcaption{Example image}
	\label{fig:sans:204}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/20_mean.jpg}
    \subcaption{Enhanced image using average noise model and 20 clusters for gradient classification}
    \label{fig:sans:204:20_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/20_cent.jpg}
    \subcaption{Enhanced image using centroids noise model and 20 clusters for gradient classification}
    \label{fig:sans:204:20_cent}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/25_mean.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:204:25_mean}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/204/25_cent.jpg}
    \subcaption{Enhanced image using average noise model and 25 clusters for gradient classification}
    \label{fig:sans:204:25_cent}
  \end{subfigure}
}

\caption{Example results of the semi-automated algorithm}
\label{fig:sans:res2}

\end{figure}


\par
The example images show that varying the algorithm parameters makes a significant difference when the input had low quality.
This can be explained with two factors.
First, high quality images have less noise on the pattern, Figures \ref{fig:sans:21} and \ref{dig:sans:204}, or the noise is less varying, Figure \ref{fig:sans:21}, than on a lower quality sample, Figure \ref{fig:sans:174}.
Second, using 20 and 25 classes is overstated, on high quality images there is no high variance between the properties of edges.
However choosing high amount of classes was an intentional decision, first the structure of the image is better preserved, when the means are calculated on a high amount of subgroups, second using fewer classes leads to eliminating valuable pattern information when deleting the members of the most populated class.
Comparing the output of the two different kinds of noise models, there is very little difference between their performance.
Both models were able to preserve pattern information where the foreground is not cluttered like on Figure \ref{fig:sans:25}.
Focusing on small details like the top of Figure \ref{fig:sans:21} the average noise model is more aggressive than the centroid one.
That observation corresponds to the calculation of the noise representation, taking the average of every noise pixel results in a less detailed model than calculating more representatives, thus it is more destructive than the centroid representation.
On the result images of Figure \ref{fig:sans:204} significant shoeprint pattern parts are missing on the bottom area, that is the combined effect of several steps of the pipeline which is based on the heuristic that small regions are noise elements rather than relevant pattern information.

\par
On Figure \ref{fig:sans:pip} the entire pipeline of the semi-automated algorithm is shown on two example images.
On Figures  \ref{fig:sans:pip:21:bg} \ref{fig:sans:pip:204:bg} and \ref{fig:sans:pip:239:bg} the binary mask is shown, where white denotes the background and black the foreground area.
Inspecting the images the majority of the background is labelled correctly, however two limitations are visible.
The first one is the bottom area of Figure \ref{fig:sans:pip:204:bg}, where half of the fine line area of the actual shoeprint is cropped.
The other one is in Figures \ref{fig:sans:pip:21:bg} and \ref{fig:sans:pip:239:bg}, where because of the significant change within the background the border area is labelled as foreground.
The appearance of the input image after eliminating the noise in the foreground is shown in the third column.
Figures  \ref{fig:sans:pip:204:bg} and \ref{fig:sans:pip:239:bg} illustrate that the noise suppression technique does not damage the pattern area, the difference between the fore- and the background is preserved.
On Figure \ref{fig:sans:pip:21} there is significant noise on the top area of the foreground, which is successfully suppressed on Figure \ref{fig:sans:pip:21:enh}.
In the next step non-local means are calculated based on the differences of the gradients.
Focusing on the background of the images in the fourth column, nearly homogen background is created without applying the noise mask on the images.
However, on Figure \ref{fig:sans:pip:204:grad} the biggest weakness of the algorithm is also visible.
The non-local mean calculation eliminates the majority of pattern information in both top and bottom area of the original shoeprint.
Combined with highly structured background noise, several mixed classes are generated where noise and pattern data occur simultaneously.
The artifacts of this phenomenon are visible not only on the said shoeprint pattern area, but also in the middle of the image, where noise structures were enhanced appearing as bright lines similar to the remaining pattern edges.
Figures \ref{fig:sans:pip:21:th}, \ref{fig:sans:pip:204:th} and \ref{fig:sans:pip:239:th} show the binarized images after applying the background mask.
Because of the nearly homogen background achieved with non-local means the thresholding method is able to preserve the extracted lines.
Masking the binarized image, falsely clustered edges, such as the ones in the middle of Figure \ref{fig:sans:pip:204:grad}, are deleted easily.
In the last column the final results are shown after the postprocessing step of eliminated small regions.
Comparing Figure \ref{fig:sans:pip:21:th} to  Figure \ref{fig:sans:pip:21:res} the latter is less cluttered, many small lines around the pattern contours were eliminated successfully while no relevant edges were deleted.
The same effect is also visible on Figure \ref{fig:sans:pip:239:res} where the silhouette of the pattern is clearer since the majority of remaining line noise was eliminated.
However on Figure \ref{fig:sans:pip:204:res} the pattern information is further damaged by the postprocessing step.
Observing the right side of the bottom area it is clearly visible that several pattern contours were eliminated.
The postprocessing step assumes that the shoeprint consists of bigger structures such as filled geometries on Figure \ref{fig:sans:pip:21} or broad edges on Figure \ref{fig:sans:pip:239}, thus the detailed edge pattern of Figure \ref{fig:sans:pip:204} is automatically considered as noise instead of relevant information.

\begin{figure}[h]

\subfloat{
  \centering
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/orig.jpg}
    \subcaption{Example image}
	\label{fig:sans:pip:21}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/mask.jpg}
    \subcaption{Background mask}
    \label{fig:sans:pip:21:bg}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/enhance.jpg}
    \subcaption{Enhanced image after eliminating noise on the foreground}
    \label{fig:sans:pip:21:enh}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/grad.jpg}
    \subcaption{Gradient image after calculating non-local means}
    \label{fig:sans:pip:21:grad}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/th.jpg}
    \subcaption{Binarized image using adaptive thresholding}
    \label{fig:sans:pip:21:th}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/21/res.jpg}
    \subcaption{Resulting image after eliminating small structures}
    \label{fig:sans:pip:21:res}
  \end{subfigure}
}

\subfloat{
  \centering
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/orig.jpg}
    \subcaption{Example image}
	\label{fig:sans:pip:204}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/mask.jpg}
    \subcaption{Background mask}
    \label{fig:sans:pip:204:bg}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/enhance.jpg}
    \subcaption{Enhanced image after eliminating noise on the foreground}
    \label{fig:sans:pip:204:enh}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/grad.jpg}
    \subcaption{Gradient image after calculating non-local means}
    \label{fig:sans:pip:204:grad}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/th.jpg}
    \subcaption{Binarized image using adaptive thresholding}
    \label{fig:sans:pip:204:th}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/204/res.jpg}
    \subcaption{Resulting image after eliminating small structures}
    \label{fig:sans:pip:204:res}
  \end{subfigure}
}

\subfloat{
  \centering
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/orig.jpg}
    \subcaption{Example image}
	\label{fig:sans:pip:239}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/mask.jpg}
    \subcaption{Background mask}
    \label{fig:sans:pip:239:bg}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/enhance.jpg}
    \subcaption{Enhanced image after eliminating noise on the foreground}
    \label{fig:sans:pip:239:enh}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/grad.jpg}
    \subcaption{Gradient image after calculating non-local means}
    \label{fig:sans:pip:239:grad}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/th.jpg}
    \subcaption{Binarized image using adaptive thresholding}
    \label{fig:sans:pip:239:th}
  \end{subfigure}
  \begin{subfigure}[b]{0.16\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/pipeline/239/res.jpg}
    \subcaption{Resulting image after eliminating small structures}
    \label{fig:sans:pip:239:res}
  \end{subfigure}
}

\caption{The whole pipeline of the semi-automated algorithm using 25 classes and centroids for noise elimination on pattern}
\label{fig:sans:pip}

\end{figure}

\par
The experimental results show that the semi automated algorithm provides more detailed results, Figure \ref{fig:sans:25:25_cent}, than the firstly introduced modified three-layered feature learning approach, Figure \ref{fig:pe:25:FM}.
Furthermore it copes better with noise, Figure \ref{fig:sans:pip:21:res}, than the fully-automated method, Figure \ref{fig:fans:21:out}.
Despite the fact that the fully-automated algorithm achieves better results on detailed, fine lined shoeprints impressions, Figures \ref{fig:fans:235:out} and \ref{fig:fans:235:out} then the semi-automated one, Figure \ref{fig:sans:pip:204:res}, overall the latter has higher performance than the two previous ones because it is able to deal with low-quality samples, Figure \ref{fig:sans:174:25_cent}, as well.
However, along the weakness of misinterpreting fine edges there is an additional limitation of the proposed method.
When there is low contrast between the background and foreground, or the foreground is highly cluttered, no robust estimation can be made about the background area and about the dominant appearance of noise.
A such case is shown on Figure \ref{fig:sans:noise}.
The output is computed solely based on the gradient-based non-local means method and the postprocessing step.
When the calculated background area is too small or too big, no masking is made after binarizing the image and no noise is eliminated from the foreground since no model about the clutter on the image is calculated.
In such cases several enhancement steps are not exploited, thus the quality of the output is lower.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.3\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noise/orig.jpg}
    \subcaption{Example image from the dataset}
    \label{fig:sans:noise:orig}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{5/noise/mask.jpg}
    \subcaption{Calculated background mask}
    \label{fig:sans:noise:mask}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\columnwidth}
    \centering
   \includegraphics[width=\textwidth]{5/noise/res.jpg}
    \subcaption{Output of the semi-automated algorithm}
    \label{fig:sans:noise:res}
  \end{subfigure}
 
  \caption{Output of the semi-automated algorithm, where no estimation about the background can be made}
  \label{fig:sans:noise}
\end{figure}

\par
Based on the evaluations in the last three chapters the semi-automated approach proved to be the most powerful technique from the three proposed algorithms.
To get a detailed overview about its performance, additional tests are conducted in the following chapter, and the results are further discussed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Results and Discussion}
\par
In this chapter the experimental results of the semi-automated approach conducted on a minimal dataset from FID-300 are presented and discussed.
The testing database consist of 20 images, the selected images and the output of the algorithm are shown in the Appendix \ref{AppA}.
When selecting the testing dataset, no restrictions were made, low quality and heavily cluttered images are also part of the experimental database.
Since the evaluation in the previous chapter showed that the semi-automated approach is not effective on samples with detailed, fine line structures, those images \ref{appA:204}, \ref{appA:223}, \ref{appA:241} are excluded while testing.
Furthermore it was mentioned as well that in case of low intensity difference no background estimation is possible. For that reason images \ref{appA:217} and \ref{appA:232} are also disregarded while evaluation.
\par
The visual improvement caused by the different algorithms were already discussed in the previous chapters.
Now the analytical results of the semi-automated method are analyzed based on the performance of three basic feature descriptors.
The goal of the algorithm is shoeprint enhancement and to increase the overall sample quality and not to develop a powerful descriptor or a matching technique.
Hence for evaluation three basic descriptors were chosen, Fourier-Mellin features, SIFT and SURF, which are popular techniques for shoeprint, fingerprint and for natural image description.
Fourier-Mellin transform was proposed for shoeprint description several times \cite{gueham2008automatic}, \cite{richetelli2017classification}, \cite{wu2019crime}.
Along shoeprint description \cite{nibouche2009rotation}, \cite{richetelli2017classification} SIFT is also favored for fingerprint \cite{zhou2011adaptive} and for tattoo identification \cite{yi2015impact}, \cite{han2013tattoo}.
Although SURF is not common for shoeprint recognition it is popular in related areas such as fingerprint description \cite{jahan2017robust}, tattoo matching \cite{yi2015impact} and natural texture description \cite{prabhakar2012lbp}.
The evaluation is done by comparing the matching performance of those descriptors on the original samples and on the processed images.
If the basic descriptors, without embedding in a more complex pipeline, achieve higher accuracy on the processed  images than on the original ones, it implies successful enhancement.
The experimental results already conducted on the field indicate \cite{rida2019forensic} that using a complex approach increase matching accuracy, however when only one given method is used for evaluation it is not clear if the enhancement is optimized for that specific pipeline or is generally valid.
Evaluating based on the feature descriptors which are the core parts of many image matching algorithms provides general results on the performance.
\par
For matching the extracted features two algorithms are used Brute-Force matching (BF) and Fast Library for Approximate Nearest Neighbors (FLANN).
BF simply compares a feature of one image to every other features of the other one and sets the closest one as match, whereas FLANN picks the best matches from the neighborhood, it does not examine all possible feature point so it is faster than BF.
The criteria for 'good match' is set to 0.8 while testing.
For matching the shoeprint samples the Ground Truth images of every member of the minimal database were selected and stored, and it is examined how many 'good matches' were found between the samples and the Ground Truth images.
At the end of the matching process a ranking among the Ground Truth images is made, the one with the most 'good matches' is the first, the one with the second most 'good matches' is the second and so on.
The ranking of the actual Ground Truth of the given sample is noted and the final score is made by averaging the noted ranking values  of every image from the testing dataset.
That means if the belonging Ground Truth image of every member of the dataset had the most 'good matches', thus the highest ranking, the final score is 1.
Lower performance is indicated with higher score, correct matching results in lower score.
\par
To create a baseline the original samples were matched to the Ground Truth images \ref{tab:BL}.
For comparison all four possible outputs of the enhancing algorithm is presented in Tables \ref{tab:25Mean}, \ref{tab:25Cent} and \ref{tab:20Mean}, \ref{tab:20Cent}.
Table \ref{tab:ov} gives an overview about the overall performance of the discussed pipelines, in the upper side of the rows the calculated average score of both matching techniques is shown in Tables \ref{tab:25Mean}, \ref{tab:25Cent} and \ref{tab:20Mean}, \ref{tab:20Cent} and \ref{tab:BL}.
In the lower part of the rows the average score of both matching techniques is displayed.
In the last column the average score of all descriptors using both matching techniques is shown.
To understand the performance of the proposed enhancing method this table is analyzed in the following.
\par
Focusing on the first (F-M) and third (SURF) column of the table, the matching of the enhanced shoeprints always outperforms the original samples regardless the pipeline settings, except one case when FLANN matching is used on the output of 20 classes and average noise model.
However, considering the second column (SIFT) of the table that is not the case, and the enhanced images are not able to achieve the performance of the original images.
Wang et al. \cite{wang2017manifold} state that SIFT is less effective on binarized images, since it seeks less information about the structure of the image than in the original version.
On the other hand they also mention that SURF suffers from the same limitation, which is not the case in our experiments.
This contradiction is explainable by examining the working pipeline of SURF, it uses a blob-detector to determine feature points which is suitable for shoeprint identification.
The majority of shoeprints contain closed structures such as circles or polygons that are easily recognized by a blob-detector.
Thus even though working on a binary image is not ideal for SIFT nor for SURF, the latter is able to overcome its weakness by taking advantage of the specific features of a shoe sole pattern.

\begin{minipage}{\linewidth}
\centering
\begin{tabular}{ccccccc}
& \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
1 & 12 & 13 & 9 & 4 & 1 & 1 \\
2 & 4 & 5 & 1 & 3 & 2 & 2 \\
3 & 5 & 7 & 2 & 1 & 4 & 4 \\
4 & 5 & 6 & 1 & 1 & 5 & 2 \\
5 & 6 & 1 & 1 & 1 & 5 & 4 \\
6 & 6 & 6 & 4 & 6 & 6 & 6 \\
7 & 6 & 6 & 2 & 8 & 8 & 9 \\
8 & 3 & 4 & 4 & 3 & 8 & 8 \\
9 & 2 & 1 & 7 & 4 & 7 & 7 \\
10 & 9 & 10 & 12 & 9 & 13 & 12 \\
11 & 2 & 1 & 12 & 13 & 11 & 11 \\
16 & 6 & 5 & 1 & 1 & 9 & 6 \\
17 & 1 & 1 & 6 & 5 & 1 & 1 \\
18 & 13 & 11 & 12 & 7 & 12 & 12 \\
20 & 8 & 11 & 6 & 4 & 7 & 3 \\
Score & 5.866666667 & 5.866666667 & 5.333333333 & 4.666666667 & 6.6 & 5.866666667 \\
\end{tabular}
\captionof{table}{Baseline results matching the original images with the unprocessed Ground Truth images}
 \label{tab:BL} 
\end{minipage}


\begin{minipage}{\linewidth}
\centering
\begin{tabular}{ccccccc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
1 & 1 & 1 & 2 & 8 & 4 & 9 \\
2 & 1 & 1 & 11 & 10 & 1 & 2 \\
3 & 3 & 4 & 1 & 1 & 6 & 7 \\
4 & 2 & 3 & 8 & 10 & 7 & 8 \\
5 & 1 & 2 & 5 & 3 & 4 & 4 \\
6 & 5 & 9 & 2 & 3 & 6 & 8 \\
7 & 13 & 12 & 13 & 13 & 8 & 8 \\
8 & 2 & 2 & 12 & 11 & 8 & 8 \\
9 & 8 & 11 & 13 & 6 & 4 & 4 \\
10 & 4 & 9 & 7 & 3 & 1 & 1 \\
11 & 2 & 2 & 6 & 8 & 12 & 11 \\
16 & 5 & 6 & 1 & 1 & 4 & 6 \\
17 & 2 & 3 & 7 & 10 & 1 & 2 \\
18 & 6 & 7 & 1 & 1 & 7 & 2 \\
20 & 5 & 2 & 4 & 6 & 3 & 2 \\
Score & 4 & 4.933333333 & 6.2 & 6.266666667 & 5.066666667 & 5.466666667 \\
\end{tabular}
\captionof{table}{Results using 25 classes and the average noise model}
 \label{tab:25Mean} 
\end{minipage}


\begin{minipage}{\linewidth}
\centering
\begin{tabular}{ccccccc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
1 & 2 & 1 & 5 & 6 & 1 & 1 \\
2 & 1 & 1 & 2 & 3 & 1 & 2 \\
3 & 6 & 9 & 1 & 1 & 2 & 5 \\
4 & 1 & 1 & 8 & 10 & 4 & 2 \\
5 & 5 & 7 & 1 & 2 & 2 & 3 \\
6 & 4 & 6 & 1 & 1 & 3 & 3 \\
7 & 13 & 11 & 13 & 13 & 8 & 8 \\
8 & 3 & 2 & 12 & 11 & 8 & 8 \\
9 & 8 & 11 & 13 & 6 & 4 & 4 \\
10 & 7 & 6 & 10 & 8 & 4 & 2 \\
11 & 4 & 4 & 9 & 8 & 10 & 5 \\
16 & 4 & 4 & 2 & 8 & 6 & 7 \\
17 & 6 & 5 & 5 & 12 & 1 & 2 \\
18 & 7 & 7 & 3 & 3 & 1 & 1 \\
20 & 2 & 1 & 12 & 9 & 1 & 1 \\
Score & 4.866666667 & 5.066666667 & 6.466666667 & 6.733333333 & 3.733333333 & 3.6 \\
\end{tabular}
\captionof{table}{Results using 25 classes and the centroids noise model}
 \label{tab:25Cent} 
\end{minipage}

\begin{minipage}{\linewidth}
\centering
\begin{tabular}{ccccccc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
1 & 1 & 1 & 5 & 8 & 3 & 6 \\
2 & 1 & 1 & 13 & 13 & 3 & 5 \\
3 & 1 & 2 & 1 & 1 & 5 & 6 \\
4 & 3 & 5 & 8 & 9 & 1 & 3 \\
5 & 3 & 7 & 3 & 3 & 1 & 2 \\
6 & 10 & 13 & 2 & 3 & 7 & 7 \\
7 & 13 & 12 & 13 & 13 & 8 & 8 \\
8 & 7 & 9 & 13 & 12 & 4 & 4 \\
9 & 8 & 11 & 13 & 6 & 4 & 4 \\
10 & 2 & 5 & 7 & 5 & 1 & 1 \\
11 & 3 & 2 & 8 & 8 & 10 & 7 \\
16 & 2 & 2 & 9 & 6 & 5 & 5 \\
17 & 3 & 2 & 9 & 3 & 1 & 3 \\
18 & 10 & 9 & 1 & 3 & 10 & 10 \\
20 & 4 & 4 & 4 & 6 & 2 & 2 \\
Score & 4.733333333 & 5.666666667 & 7.266666667 & 6.6 & 4.333333333 & 4.866666667 \\
\end{tabular}

\captionof{table}{Results using 20 classes and the average noise model}
 \label{tab:20Mean} 
\end{minipage}

\begin{minipage}{\linewidth}
\centering
\begin{tabular}{ccccccc}
\multirow{2}{*}{ } & \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } \\
& BF & FLANN & BF & FLANN & BF & FLANN \\
1 & 2 & 3 & 6 & 7 & 1 & 1 \\
2 & 1 & 1 & 3 & 4 & 3 & 3 \\
3 & 1 & 3 & 2 & 3 & 4 & 5 \\
4 & 1 & 1 & 11 & 9 & 5 & 5 \\
5 & 3 & 6 & 7 & 5 & 7 & 8 \\
6 & 8 & 8 & 3 & 3 & 1 & 2 \\
7 & 13 & 12 & 13 & 13 & 8 & 8 \\
8 & 7 & 9 & 13 & 12 & 4 & 4 \\
9 & 8 & 10 & 13 & 6 & 4 & 4 \\
10 & 7 & 6 & 12 & 9 & 1 & 1 \\
11 & 1 & 1 & 6 & 8 & 6 & 7 \\
16 & 4 & 2 & 1 & 3 & 9 & 9 \\
17 & 5 & 5 & 9 & 9 & 1 & 2 \\
18 & 5 & 3 & 4 & 2 & 8 & 3 \\
20 & 6 & 6 & 8 & 10 & 2 & 2 \\
Score & 4.8 & 5.066666667 & 7.4 & 6.866666667 & 4.266666667 & 4.266666667 \\
\end{tabular}
\captionof{table}{Results using 20 classes and the centroids noise model}
 \label{tab:20Cent} 
\end{minipage}

\begin{sidewaystable}
\centering
\begin{tabular}{cccccccc}
& \multicolumn{2}{c}{ F-M } & \multicolumn{2}{c}{ SIFT } & \multicolumn{2}{c}{ SURF } & \multirow{2}{*}{ Average Results } \\
& BF & FLANN & BF & FLANN & BF & FLANN & \\
Baseline & 5.655555556 & 5.238888889 & 6.027777778 & 5.472222222 & 7.8 & 7.072222222 & \multirow{2}{*}{ 6.211111111 } \\
& \multicolumn{2}{c}{ 5.447222222 } & \multicolumn{2}{c}{ 5.75 } & \multicolumn{2}{c}{ 7.436111111 } & \\
25 Average & 4 & 4.933333333 & 6.2 & 6.266666667 & 5.066666667 & 5.466666667 & \multirow{2}{*}{ 5.322222222 } \\
& \multicolumn{2}{c}{ 4.466666667 } & \multicolumn{2}{c}{ 6.233333333 } & \multicolumn{2}{c}{ 5.266666667 } & \\
25 Centroids & 4.866666667 & 5.066666667 & 6.466666667 & 6.733333333 & 3.733333333 & 3.6 & \multirow{2}{*}{ 5.077777778 } \\
& \multicolumn{2}{c}{ 4.966666667 } & \multicolumn{2}{c}{ 6.6 } & \multicolumn{2}{c}{ 3.666666667 } & \\
20 Average & 5.43912037 & 5.567283951 & 6.634259259 & 5.904320988 & 6.630787037 & 6.111728395 & \multirow{2}{*}{ 6.047916667 } \\
& \multicolumn{2}{c}{ 5.50320216 } & \multicolumn{2}{c}{ 6.269290123 } & \multicolumn{2}{c}{ 6.371257716 } & \\
20 Centorids & 4.8 & 5.066666667 & 7.4 & 6.866666667 & 4.266666667 & 4.266666667 & \multirow{2}{*}{ 5.444444444 } \\
& \multicolumn{2}{c}{ 4.933333333 } & \multicolumn{2}{c}{ 7.133333333 } & \multicolumn{2}{c}{ 4.266666667 } & \\
\end{tabular}
\captionof{table}{Overview about the performance of the analyzed pipelines}
 \label{tab:ov} 
\end{sidewaystable}



\par
Examining the first row of Table \ref{tab:ov}, the best score the original shoeprints achieved is 5.24 (BF matching of Fourier-Mellin features), whereas the worst accuracy is 7.80 (BF matching of SURF features).
The best accuracy of the baseline is outperformed several times, the Fourier-Mellin features achieve better accuracy regardless the matching technique in every case except when working with 20 classes and average model noise model, where the scores are minimally lower, 5.44 for BF and 6.57 for FLANN.
SURF features achieve the overall best scores for matching the enhanced images.
Matching the results produced by using 20 classes and centroids as noise model the score of 4.27 is noted for both BF and FLANN.
But in case of output generated by 25 classes and centroids as noise models a better value of 3.73 and 3.60 is achieved.
Furthermore, the worst performance of the baseline algorithm is 7.80, which is outperformed in every test case.
The worst score noted for the enhanced image matching is 7.40 which was provided by SIFT BF matching of the samples processed by the setting with 20 classes and using centroid models.

\par
Considering the overall accuracy (last column) of the different settings, the enhanced matching always provide better results than the baseline.
Comparing the scores of the enhanced samples to each other the pipeline using only 20 classes and the averaged noise model has the lowest performance.
Using 20 classes but the centroids for noise representation results in score improvement of above 0.5 points achieving 5.44.
The best scores were reached by the images where 25 classes were used earning 5.32 and 5.08 points, for average and centroid noise models respectively.
These values show that the detailed algorithms lead to higher quality enhancement.
Using 25 classes outperforms the case when the differences of gradients are grouped into 20 classes.
Building a more versatile model for noise representation using the centroids of the k-means algorithm results in better scores than the images enhanced with the average noise model have.
The fact that both settings using 25 classes outperformed the ones with less classes implies that detailed clustering of extracted edges is more important that the  choice of noise representation in noise suppression step.

\par
The experimental results presented in this chapter show that the proposed image enhancement algorithm supports accuracy improvement.
The performance of the algorithm was tested on a versatile subset of images selected from the FID-300 dataset.
However it has to be noted that the proposed method needs further improvement to be able to handle all possible samples of the FID-300 dataset.
For known limitations of the algorithm samples with detailed, fine line structures and low contrast between fore- and background were excluded from testing.
The matching based on the Fourier-Mellin features is the most robust one, it outperforms the baseline algorithm never having worse scores than 5.57 and the difference between the best- and worst case scenarios is only 1.57.
SURF provides the best score in given cases but it also reaches above 6.00 scores multiple times, thus the difference between the best and worst accuracy is above 3.6.
The robustness and the high-quality of Fourier-Mellin features is promising, since many current shoeprint matching algorithms already use it for feature description  \cite{gueham2008automatic}, \cite{richetelli2017classification}, \cite{wu2019crime} or are based on other frequency domain descriptors \cite{algarni2008novel}, \cite{wang2014automatic}, \cite{katireddy2017novel}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Future Work}
\par
Forensic shoeprint identification is an extensively studied research area \cite{rida2019forensic}, nevertheless there are several possibilities for improvement.
One problem of the field is the lack of uniform datasets, in many cases narrow datasets with low variability and high quality samples were used  \cite{rida2019forensic}.
Additionally the testing data is not necessarily made public, so no fast comparison between existing approaches is possible.
\par
To establish an overview and structure our knowledge in this field three prototypical approaches were introduced in this thesis
Being prototypical approaches there are several ways for improvement.
The first, most obvious direction is the combination of the three techniques.
Even though all three methods have different pipeline applying all of them simultaneously on the input data is not impossible.
Additionally the discussed advantage of one technique can suppress the limitation of the other two.
The semi-automated pipeline is currently the most powerful approach from the introduced one.
However being semi-automated and not being able to cope with fine line structures is nevertheless a significant limitation.
On the other hand letting the modified three-layered feature learning algorithm to determine the approximate outline of the pattern, and determining the exact background based on the semi-automated method replaces the now necessary user input at the beginning of the application.
Additionally, the introduced fully automated method failed on the majority of samples, it only provided reasonable output if the input had high quality and detailed line structure.
That is however exactly the case that the semi-automated pipeline is not able to handle.
Enhancing any kind of input data with the semi-automated pipeline, calculating the background mask and applying the non-local means for edge detection, and finishing the calculation with the fully automated approach in case of fine-lined shoeprint pattern solves this kind of limitation.
The preprocessing done with the semi-automated method increases the quality that the fully-automated noise elimination approach needs, and the latter one is able to identify the detailed pattern which the semi-automated technique eliminates immediately.
\par
Additionally there several approaches in the research available which were not discussed in this work yet.
Chen et al. \cite{chen2013hierarchical} introduced a hierarchical voting score for finger- and palmprint identification.
Wu et al. \cite{wu2019losgsr} also proposed a voting technique for real-life forensic images incorporating the opinion of human experts.
They defined a manifold ranking method based on the shoeprint identification of forensic specialists.
Along the judgements of the experts the neighborhood and the coefficient matrix of the given sample is also considered to develop a powerful feature descriptor.
The descriptor is based on the Fourier-Mellin Transform similar to the approaches previously introduced, thus a combination is relatively easy.
\par
Voting scores and feature-learning are machine learning techniques but deep learning \cite{lecun2015deep} were not considered either.
Rida et al. \cite{rida2019forensic} discuss the possibility of using Convolutional Neural Network (CNN) for end-to-end shoeprint recognition and identification introduced in \cite{lecun1998gradient}.
Kong et al. \cite{kong2017cross}, \cite{kong2019cross} already published an approach for taking advantage on CNNs.
However, instead of using the CNN to identify the given shoeprint they extract mid-level features of the network to use them as feature descriptor.
\par
In all approaches proposed in this thesis the geometrical location of the extracted features is disregarded, even though it is an important information while identifying shoeprint impressions.
Despite that several samples depict only a partial image as soon as there are at least two different patterns or a recognizable outline part of the shoe is visible, assumptions are possible about the geometrical location of the given pattern.
Li et al. \cite{li2015secondary} propose a subdivision method where four regions of a shoeprint are distinguished, these are cap, sole, arch and heel and processed according to their position.
Additionally the overall silhouette of a shoeprint also contains information.
Wang et al. \cite{wang2014automatic} developed a shoeprint contour model for shoeprint retrieval on high-quality samples, which helps to make their approach more robust, since it is able to correct distortions or noise on the input images.
\par
Lastly Randomly Acquired Features (RAC) of shoeprints were already mentioned before but were not considered during feature extraction for two reasons.
First, in this thesis prototypical applications are shown and the main goal is to get a basic idea about the effectiveness of the published approaches.
Second, as Rida et al. \cite{rida2019forensic} states in many dataset only a limited amount of samples is available for one shoeprint, and that is also the case in the FID-300 database.
There are no more than six images from the same shoeprint in exceptional cases, but the majority of the dataset consist of single samples.
But assuming that the demand for wider dataset increases with the active research, considering RAC features of the input provides valuable additional information about the given sample.
Shor et al. \cite{shor2018inherent} introduce a technique to distinguish between the impressions of the same shoe based on the obtained RAC features.
Damary et al. \cite{damary2018dependence} published a study about the estimated properties of a RAC based on its location of the shoeprint.
In this way if a RAC is identified assumptions about the neighboring pattern are made.
Similar to basic structure description for shoeprints \cite{tang2010footwear} a technique for RAC description was also developed \cite{speir2016quantifying} thus incorporating RAC models into shoeprint descriptors is already possible.
\par
Considering the general research done on the topic of shoeprint enhancement there are two connected problems which have to be solved in the future.
The first one is the already mentioned non-existing uniform database.
The current practice of using several datasets has several drawbacks.
The proposed results are not meaningful and consequential, since the used datasets are not public in many cases and because the results are not comparable to other methods.
Furthermore the real-life performance and usefulness of the published methods is unknown because they are often tested on synthetic and on handcrafted data.
The second problem is strongly connected to the absence of uniform database and that is the lack of overview and structured knowledge in the field.
Rida et al. \cite{rida2019forensic} recently proposed a survey about the research done in the last 20 years and made an overview about the accuracy of the published methods.
Several authors claimed to have above 99\% \cite{algarni2008novel}, \cite{alizadeh2017automatic}, \cite{almaadeed2015partial}, or exactly 100\% accuracy \cite{graham2007automatic}, on their testing datasets.
These publications imply that the problem is already solved, and there is no further room for improvement which is not the case.
Until no central, uniform, high-quality and wide dataset is available no research is done efficiently, it is not possible to take advantage of the already published approaches and it is difficult to learn from the mistakes previously done.

\chapter{Conclusion}
\par
Forensic shoeprint image retrieval and identification is an extensively researched topic in the last 20 years.
A great amount of publications is available and many methods were introduced for best shoeprint pattern recognition.
However, no structured knowledge is available on this field, thus results are uncertain, the solved and open problems are unclear and the publications are incoherent.
Because of the absence of universal, high-quality dataset the algorithms are not comparable, the limitations and strengths of the approaches are unclear and it is ambiguous if they are applicable in real-life use-cases as well.
\par
In 2014 a new database, called FID-300, was introduced by Kortylewski et al. \cite{kortylewski2014unsupervised} which is a step in the right direction, containing 300 shoeprint impressions collected on real-life crime scenes.
In this thesis an overview and introduction to possible ways for real-life shoeprint enhancement was given.
Based on the available shoeprint recognition literature and research done in related fields such as fingerprint and tattoo identification and natural image enhancement, three prototypical algorithms were presented to increase the quality of the real-life samples.
The first one is based on the idea of feature learning, the second one utilizes a traditional image enhancement pipeline and the idea between the third one is the calculation of a noise representation model and gradient clustering for relevant edge detection.
The evaluation shows that many preprocessing techniques popular in the current forensic research are not robust and powerful enough to process real-life shoeprint images.
All three algorithms are able to achieve reasonable results on specific samples, however many images with given properties, such as low quality images, overly cluttered shoeprints low-contrast pictures are crucial limitations of all of them.
Based on the visual results of all three algorithms the last one is the most robust one.
The analytical testing conducted on basic image descriptors already used in forensic image identification shows that the enhanced images achieve better matching results than the original ones in almost every cases.
With further development and potential combination of the proposed method a powerful and versatile preprocessing pipeline can be built which is able to enhance real-life shoeprint impressions and thus increase the matching accuracy.


% Remove following line for the final thesis.
%\input{intro.tex} % A short introduction to LaTeX.

\backmatter

% Use an optional list of figures.
%\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
\cleardoublepage % Start list of tables on the next empty right hand page.
%\listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of alogrithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
%\printindex

% Add a glossary.
%\printglossaries

% Add a bibliography.
\bibliographystyle{alpha}
\bibliography{intro}

\begin{appendices}
		\chapter{Appendix A}
		\label{AppA}
		This section contains the images selected for testing.
		The samples was chosen from FID-300 and are representative for the database, no restriction were made while selecting the images, low quality images are also part of the experimental setting.
		In the frist column the original images are shown, in the second and third columns the results using 20 classes for classification of the gradient difference image is visible, whereas in the fourth and fifth column the output with 25 classes is displayed.
		The results using the mean noise model are in columns two and four, the output using the centroids are in columns three and five.
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00009.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00009_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00009_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00009_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00009_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00017.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00017_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00017_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00017_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00017_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00020.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00020_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00020_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00020_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00020_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00021.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00021_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00021_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00021_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00021_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00025.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00025_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00025_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00025_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00025_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00066.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00066_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00066_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00066_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00066_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00171.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00171_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00171_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00171_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00171_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00174.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00174_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00174_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00174_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00174_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00178.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00178_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00178_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00178_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00178_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00182.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00182_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00182_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00182_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00182_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00197.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00197_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00197_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00197_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00197_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00204.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00204_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00204_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00204_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00204_filtered.jpg}
  \end{subfigure}
\caption{}
  \label{appA:204}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00217.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00217_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00217_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00217_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00217_filtered.jpg}
  \end{subfigure}
\caption{}
\label{appA:217}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00223.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00223_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00223_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00223_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00223_filtered.jpg}
  \end{subfigure}
\caption{}
 \label{appA:223}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00232.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00232_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00232_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00232_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00232_filtered.jpg}
  \end{subfigure}
\caption{}
\label{appA:232}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00233.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00233_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00233_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00233_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00233_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00235.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00235_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00235_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00235_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00235_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00239.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00239_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00239_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00239_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00239_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00241.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00241_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00241_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00241_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00241_filtered.jpg}
  \end{subfigure}
	\caption{}
  \label{appA:241}
\end{figure}  

\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/00250.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/mean/00250_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k20/centroids/00250_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/mean/00250_filtered.jpg}
  \end{subfigure}
  \begin{subfigure}[b]{0.19\columnwidth}
    \centering
    \includegraphics[width=\textwidth]{appA/k25/centroids/00250_filtered.jpg}
  \end{subfigure}
\caption{}
\end{figure}  

\end{appendices}

\end{document}